"""GraphRAG æ£€ç´¢æœåŠ¡ï¼šå›¾æ•°æ®åº“ + å‘é‡æ£€ç´¢å¢å¼ºç”Ÿæˆ"""
import logging
import re
import json
import asyncio
import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from enum import Enum
from sentence_transformers import SentenceTransformer
from app.core.milvus_client import milvus_client
from app.core.neo4j_client import neo4j_client
from app.core.config import settings
from app.services.rag_settings import (
    RAG_RELEVANCE_SCORE_THRESHOLD,
    RAG_COLLECTION_NAME,
    RAG_EMBEDDING_MODEL_NAME,
    RAG_DEFAULT_TOP_K,
    EMBEDDING_CACHE_MAX_SIZE,
    VECTOR_SEARCH_CACHE_MAX_SIZE,
    EMBEDDING_CACHE_TTL_SECONDS,
    VECTOR_SEARCH_CACHE_TTL_SECONDS,
    CACHE_STATS_LOG_EVERY_N_CALLS,
    MILVUS_METRIC_TYPE,
    MILVUS_NPROBE,
)

logger = logging.getLogger(__name__)


class QueryIntent(Enum):
    """æŸ¥è¯¢æ„å›¾ç±»å‹"""
    ROUTE = "route"  # è·¯çº¿/è¡Œç¨‹æ¨è
    LISTING = "listing"  # åˆ—è¡¨/æ•°é‡æŸ¥è¯¢
    DETAIL = "detail"  # è¯¦æƒ…/ä»‹ç»æŸ¥è¯¢
    COMPARISON = "comparison"  # æ¯”è¾ƒç±»æŸ¥è¯¢
    LOCATION = "location"  # ä½ç½®/å¯¼èˆªæŸ¥è¯¢
    FEATURE = "feature"  # ç‰¹è‰²/åŠŸèƒ½æŸ¥è¯¢
    GENERAL = "general"  # é€šç”¨æŸ¥è¯¢


def _monotonic() -> float:
    return time.monotonic()

def _strip_emoji(text: str) -> str:
    """å»æ‰è¡¨æƒ…ä¸æœ«å°¾æ§åˆ¶å­—ç¬¦ï¼Œé¿å… TTS å¼‚å¸¸ï¼›ç¼ºå¥å°¾æ—¶è¡¥å¥å·ã€‚"""
    if not text or not isinstance(text, str):
        return text or ""
    s = re.sub(
        r"[\u2600-\u26FF\u2700-\u27BF"
        r"\U0001F300-\U0001F9FF"
        r"\U0001FA00-\U0001FAFF"  # newer emoji blocks (e.g., ğŸ«¶)
        r"]",
        "",
        text,
    )
    s = re.sub(r"[~\uFF5E\u301C]", "", s)
    s = re.sub(r"\s{2,}", " ", s).strip()
    s = re.sub(r"[\s\u200b\u200c\u200d\ufeff\r\n]+$", "", s)
    if s and s[-1] not in "ã€‚ï¼ï¼Ÿ.!?â€¦":
        s = s.rstrip("ï¼Œã€ï¼›ï¼š") + "ã€‚"
    return s

def _clean_special_symbols(text: str) -> str:
    """æ¸…ç†ç‰¹æ®Šç¬¦å·å’Œ Markdown æ ¼å¼ï¼Œç¡®ä¿è¾“å‡ºä¸ºçº¯æ–‡æœ¬"""
    if not text or not isinstance(text, str):
        return text or ""
    s = text
    # ç§»é™¤ Markdown ç²—ä½“ã€æ–œä½“ç¬¦å·
    s = re.sub(r"\*\*([^*]+)\*\*", r"\1", s)  # **ç²—ä½“** -> ç²—ä½“
    s = re.sub(r"\*([^*]+)\*", r"\1", s)  # *æ–œä½“* -> æ–œä½“
    s = re.sub(r"#+\s*", "", s)  # Markdown æ ‡é¢˜ç¬¦å·
    # ç§»é™¤åˆ—è¡¨ç¬¦å·ï¼ˆä¿ç•™å†…å®¹ï¼‰
    s = re.sub(r"^[\s]*[-â€¢â–ªâ–«]\s+", "", s, flags=re.MULTILINE)
    s = re.sub(r"^[\s]*[1-9]\d*[\.ã€]\s+", "", s, flags=re.MULTILINE)  # æ•°å­—åˆ—è¡¨
    # ç§»é™¤è£…é¥°æ€§ç¬¦å·
    s = re.sub(r"[ï½~â€”â€”â€¦â€¢â–ªâ–«]+", "", s)
    # ç§»é™¤ emoji æ•°å­—ï¼ˆå¦‚ 1ï¸âƒ£ã€2ï¸âƒ£ï¼‰
    s = re.sub(r"[\u0030-\u0039]\uFE0F\u20E3", "", s)
    # ç§»é™¤å¤šä½™çš„è£…é¥°æ€§æ ‡ç‚¹
    s = re.sub(r"[ã€‚]{2,}", "ã€‚", s)
    s = re.sub(r"\s{2,}", " ", s).strip()
    return s


try:
    import jieba
    import jieba.posseg as pseg
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    logger.warning("jieba not available, using simple keyword extraction")

class RAGService:
    """GraphRAGï¼šå®ä½“è¯†åˆ« + Milvus å‘é‡æ£€ç´¢ + Neo4j å›¾æ£€ç´¢ + ç»“æœèåˆã€‚"""

    def __init__(self):
        self.embedding_model = None
        self.llm_client = None
        self._milvus_loaded_collections: set[str] = set()
        self._embedding_cache: Dict[str, Tuple[List[float], float]] = {}
        self._vector_search_cache: Dict[
            Tuple[str, str, int], Tuple[List[Dict[str, Any]], float]
        ] = {}
        self._cache_stats: Dict[str, int] = {
            "embedding_calls": 0,
            "embedding_hits": 0,
            "embedding_misses": 0,
            "vector_calls": 0,
            "vector_hits": 0,
            "vector_misses": 0,
        }
        self._init_embedding_model()
        self._init_ner()
        self._init_llm_client()

    def _log_cache_stats_if_needed(self) -> None:
        every = max(1, int(CACHE_STATS_LOG_EVERY_N_CALLS))
        total = int(self._cache_stats.get("embedding_calls", 0)) + int(
            self._cache_stats.get("vector_calls", 0)
        )
        if total <= 0 or total % every != 0:
            return

        def _rate(hit: int, call: int) -> float:
            return (hit / call) if call > 0 else 0.0

        e_calls = int(self._cache_stats.get("embedding_calls", 0))
        v_calls = int(self._cache_stats.get("vector_calls", 0))
        logger.info(
            "cache_stats: embedding_hit_rate=%.1f%% (%d/%d), vector_hit_rate=%.1f%% (%d/%d), sizes: embedding=%d, vector=%d",
            _rate(int(self._cache_stats.get("embedding_hits", 0)), e_calls) * 100,
            int(self._cache_stats.get("embedding_hits", 0)),
            e_calls,
            _rate(int(self._cache_stats.get("vector_hits", 0)), v_calls) * 100,
            int(self._cache_stats.get("vector_hits", 0)),
            v_calls,
            len(self._embedding_cache),
            len(self._vector_search_cache),
        )

    def _cache_get_embedding(self, key: str) -> Optional[List[float]]:
        item = self._embedding_cache.get(key)
        if not item:
            return None
        payload, expires_at = item
        if expires_at > 0 and _monotonic() >= expires_at:
            self._embedding_cache.pop(key, None)
            return None
        return payload

    def _cache_set_embedding(self, key: str, payload: List[float]) -> None:
        ttl = max(0, int(EMBEDDING_CACHE_TTL_SECONDS))
        expires_at = _monotonic() + ttl if ttl > 0 else 0.0
        if len(self._embedding_cache) >= EMBEDDING_CACHE_MAX_SIZE:
            try:
                first_key = next(iter(self._embedding_cache))
                self._embedding_cache.pop(first_key, None)
            except StopIteration:
                pass
        self._embedding_cache[key] = (payload, expires_at)

    def _cache_get_vector(
        self, key: Tuple[str, str, int]
    ) -> Optional[List[Dict[str, Any]]]:
        item = self._vector_search_cache.get(key)
        if not item:
            return None
        payload, expires_at = item
        if expires_at > 0 and _monotonic() >= expires_at:
            self._vector_search_cache.pop(key, None)
            return None
        return payload

    def _cache_set_vector(
        self, key: Tuple[str, str, int], payload: List[Dict[str, Any]]
    ) -> None:
        ttl = max(0, int(VECTOR_SEARCH_CACHE_TTL_SECONDS))
        expires_at = _monotonic() + ttl if ttl > 0 else 0.0
        if len(self._vector_search_cache) >= VECTOR_SEARCH_CACHE_MAX_SIZE:
            try:
                first_key = next(iter(self._vector_search_cache))
                self._vector_search_cache.pop(first_key, None)
            except StopIteration:
                pass
        self._vector_search_cache[key] = (payload, expires_at)

    async def parse_scenic_text(self, text: str) -> Optional[Dict[str, Any]]:
        """å°†æ™¯åŒºä»‹ç»ç»“æ„åŒ–ä¸º JSON ä¾›å›¾åº“å»ºç°‡ï¼›éæ™¯åŒºç±»è¿”å› Noneã€‚"""
        scenic_keywords = ["æ™¯åŒº", "é£æ™¯åŒº", "æ—…æ¸¸åº¦å‡åŒº", "æ™¯ç‚¹", "åº¦å‡åŒº"]
        if not any(k in text for k in scenic_keywords):
            return None

        if not self.llm_client:
            return None

        system_prompt = """
ä½ æ˜¯æ™¯åŒºçŸ¥è¯†ç»“æ„åŒ–åŠ©æ‰‹ã€‚è¯·æŠŠä¸€æ®µä¸­æ–‡æ™¯åŒºä»‹ç»æå–æˆ JSONï¼Œä¸¥æ ¼æŒ‰å­—æ®µè¿”å›ï¼Œä¸è¦å¤šä½™è¯´æ˜ã€‚

è¿”å›å­—æ®µï¼š
- scenic_spot: æ™¯åŒºåç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
- location: è¡Œæ”¿å±‚çº§æ•°ç»„ï¼Œä¾‹å¦‚ ["å››å·çœ", "å®œå®¾å¸‚", "é•¿å®å¿"]ï¼ˆè‹¥ç¼ºå°‘ä¸‹çº§å¯çœç•¥ï¼‰
- area: é¢ç§¯ï¼ˆåŸæ–‡ä¸­çš„æè¿°å­—ç¬¦ä¸²ï¼Œè‹¥æ²¡æœ‰åˆ™ä¸º nullï¼‰
- features: ç‰¹è‰²æ•°ç»„ï¼Œä¾‹å¦‚ ["å¤©ç„¶ç«¹æ—","æ£®æ—è¦†ç›–ç‡93%","æ°”å€™æ¸©å’Œ","é›¨é‡å……æ²›"]
- spots: å­æ™¯ç‚¹æ•°ç»„ï¼Œä¾‹å¦‚ ["ç«¹æµ·åšç‰©é¦†","èŠ±æºªåä¸‰æ¡¥","æµ·ä¸­æµ·","å¤åˆ¹"]
- awards: è£èª‰æ•°ç»„ï¼Œä¾‹å¦‚ ["å›½å®¶é¦–æ‰¹4Açº§æ—…æ¸¸åŒº","å…¨å›½åº·å…»æ—…æ¸¸åŸºåœ°","å›½å®¶çº§æ—…æ¸¸åº¦å‡åŒº"]

åªè¾“å‡º JSON å¯¹è±¡ï¼Œä¸è¦è§£é‡Šã€‚
"""
        try:
            resp = self.llm_client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text},
                ],
                temperature=0.1,
                max_tokens=512,
            )
            raw = resp.choices[0].message.content
            data = json.loads(raw)
            if not isinstance(data, dict):
                return None
            scenic_name = data.get("scenic_spot")
            if not scenic_name or not isinstance(scenic_name, str):
                return None
            data["location"] = data.get("location") or []
            data["features"] = data.get("features") or []
            data["spots"] = data.get("spots") or []
            data["awards"] = data.get("awards") or []
            return data
        except Exception as e:
            logger.warning(f"parse_scenic_text failed: {e}")
            return None

    async def parse_attraction_text(self, name: str, text: str) -> Optional[Dict[str, Any]]:
        """å°†å•æ™¯ç‚¹ä»‹ç»ç»“æ„åŒ–ä¸º JSON ä¾›å›¾åº“å»ºç°‡ã€‚"""
        if not name or not isinstance(name, str):
            return None
        if not self.llm_client:
            return None

        system_prompt = """
ä½ æ˜¯æ™¯ç‚¹çŸ¥è¯†ç»“æ„åŒ–åŠ©æ‰‹ã€‚è¯·æŠŠä¸€æ®µä¸­æ–‡â€œå•ä¸ªæ™¯ç‚¹â€çš„ä»‹ç»æå–æˆ JSONï¼Œä¸¥æ ¼æŒ‰å­—æ®µè¿”å›ï¼Œä¸è¦å¤šä½™è¯´æ˜ã€‚

è¿”å›å­—æ®µï¼š
- name: æ™¯ç‚¹åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
- location: è¡Œæ”¿å±‚çº§æ•°ç»„ï¼Œä¾‹å¦‚ ["å››å·çœ", "å®œå®¾å¸‚", "é•¿å®å¿"]ï¼ˆè‹¥æ— æ³•åˆ¤æ–­åˆ™è¿”å› []ï¼‰
- category: ç±»åˆ«ï¼ˆå­—ç¬¦ä¸²æˆ– nullï¼‰
- features: ç‰¹è‰²/è¦ç‚¹æ•°ç»„ï¼ˆè‹¥æ²¡æœ‰åˆ™ []ï¼‰
- honors: è£èª‰/ç§°å·æ•°ç»„ï¼ˆè‹¥æ²¡æœ‰åˆ™ []ï¼‰

åªè¾“å‡º JSON å¯¹è±¡ï¼Œä¸è¦è§£é‡Šã€‚
"""
        try:
            resp = self.llm_client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"æ™¯ç‚¹åç§°ï¼š{name}\n\n{text}"},
                ],
                temperature=0.1,
                max_tokens=512,
            )
            raw = resp.choices[0].message.content
            data = json.loads(raw)
            if not isinstance(data, dict):
                return None
            if data.get("name") and not isinstance(data.get("name"), str):
                return None
            data["name"] = (data.get("name") or name).strip()
            data["location"] = data.get("location") or []
            data["features"] = data.get("features") or []
            data["honors"] = data.get("honors") or []
            return data
        except Exception as e:
            logger.warning(f"parse_attraction_text failed: {e}")
            return None
    
    def _init_embedding_model(self):
        try:
            self.embedding_model = SentenceTransformer(RAG_EMBEDDING_MODEL_NAME)
            logger.info("Embedding model loaded: %s", RAG_EMBEDDING_MODEL_NAME)
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            self.embedding_model = None
    
    def _init_ner(self):
        if JIEBA_AVAILABLE:
            try:
                jieba.initialize()
                logger.info("NER model (jieba) initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize jieba: {e}")
    
    def _init_llm_client(self):
        try:
            if settings.OPENAI_API_KEY:
                import openai
                client_kwargs = {"api_key": settings.OPENAI_API_KEY}
                if settings.OPENAI_API_BASE:
                    client_kwargs["base_url"] = settings.OPENAI_API_BASE
                
                self.llm_client = openai.OpenAI(**client_kwargs)
                logger.info(f"LLM client initialized (base_url: {settings.OPENAI_API_BASE or 'default'})")
            else:
                logger.warning("OpenAI API key not configured, LLM generation disabled")
        except Exception as e:
            logger.error(f"Failed to initialize LLM client: {e}")
            self.llm_client = None
    
    def extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬æå–å®ä½“ï¼Œè¿”å› [{"text", "type", "confidence"}]ã€‚"""
        stop_words = {"è¿™é‡Œ", "é‚£é‡Œ", "å“ªäº›", "ä»€ä¹ˆ", "è¿™ä¸ª", "é‚£ä¸ª", "æ™¯ç‚¹", "æ™¯åŒº", "åœ°æ–¹", 
                      "attraction", "scenic", "spot", "è¿™é‡Œæœ‰å“ªäº›", "æœ‰å“ªäº›æ™¯ç‚¹", "æ™¯ç‚¹éƒ½æœ‰"}
        entities = []
        if JIEBA_AVAILABLE:
            words = pseg.cut(text)
            for word, flag in words:
                if word in stop_words:
                    continue
                if (flag in ['ns', 'nr', 'nt', 'nz'] or len(word) >= 3) and len(word) >= 2:
                    entities.append({
                        "text": word,
                        "type": self._map_pos_to_entity_type(flag),
                        "confidence": 0.8
                    })
        else:
            pattern = r'[\u4e00-\u9fa5]{2,}'
            matches = re.finditer(pattern, text)
            for match in matches:
                word = match.group()
                if word not in stop_words:
                    entities.append({
                        "text": word,
                        "type": "KEYWORD",
                        "confidence": 0.6
                    })
        seen = set()
        unique_entities = []
        for entity in entities:
            if entity["text"] not in seen:
                seen.add(entity["text"])
                unique_entities.append(entity)
        
        return unique_entities
    
    def _map_pos_to_entity_type(self, pos: str) -> str:
        mapping = {
            'ns': 'LOCATION', 'nr': 'PERSON', 'nt': 'ORG', 'nz': 'OTHER',
        }
        return mapping.get(pos, 'KEYWORD')
    
    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        if not self.embedding_model:
            raise ValueError("Embedding model not loaded")

        key = (text or "").strip()
        if not key:
            return []
        self._cache_stats["embedding_calls"] = int(self._cache_stats.get("embedding_calls", 0)) + 1
        cached = self._cache_get_embedding(key)
        if cached is not None:
            self._cache_stats["embedding_hits"] = int(self._cache_stats.get("embedding_hits", 0)) + 1
            self._log_cache_stats_if_needed()
            return cached
        self._cache_stats["embedding_misses"] = int(self._cache_stats.get("embedding_misses", 0)) + 1

        embedding = self.embedding_model.encode(key, convert_to_numpy=True)
        emb_list = embedding.tolist()
        self._cache_set_embedding(key, emb_list)
        self._log_cache_stats_if_needed()
        return emb_list

    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆæ¯”é€æ¡ encode æ›´å¿«ï¼‰"""
        if not self.embedding_model:
            raise ValueError("Embedding model not loaded")
        if not texts:
            return []

        keys = [(t or "").strip() for t in texts]
        results: List[List[float]] = []
        to_encode: List[str] = []
        missing_indices: List[int] = []
        for idx, key in enumerate(keys):
            if not key:
                results.append([])
                continue
            self._cache_stats["embedding_calls"] = int(self._cache_stats.get("embedding_calls", 0)) + 1
            cached = self._cache_get_embedding(key)
            if cached is not None:
                self._cache_stats["embedding_hits"] = int(self._cache_stats.get("embedding_hits", 0)) + 1
                results.append(cached)
            else:
                self._cache_stats["embedding_misses"] = int(self._cache_stats.get("embedding_misses", 0)) + 1
                missing_indices.append(idx)
                to_encode.append(key)
                results.append([])  # å ä½ï¼Œåé¢å¡«å……

        if to_encode:
            embs = self.embedding_model.encode(to_encode, convert_to_numpy=True).tolist()
            for pos, emb in zip(missing_indices, embs):
                key = keys[pos]
                self._cache_set_embedding(key, emb)
                results[pos] = emb

        self._log_cache_stats_if_needed()
        return results
    
    async def vector_search(
        self,
        query: str,
        collection_name: str = "",
        top_k: int = 0,
    ) -> List[Dict[str, Any]]:
        """å‘é‡ç›¸ä¼¼åº¦æœç´¢ã€‚"""
        collection_name = (collection_name or RAG_COLLECTION_NAME).strip()
        top_k = int(top_k or RAG_DEFAULT_TOP_K)
        if not query or not collection_name or top_k <= 0:
            return []

        cache_key = (query, collection_name, top_k)
        self._cache_stats["vector_calls"] = int(self._cache_stats.get("vector_calls", 0)) + 1
        cached = self._cache_get_vector(cache_key)
        if cached is not None:
            self._cache_stats["vector_hits"] = int(self._cache_stats.get("vector_hits", 0)) + 1
            self._log_cache_stats_if_needed()
            logger.debug(
                "vector_search cache hit: collection=%s, top_k=%d", collection_name, top_k
            )
            return [dict(item) for item in cached]
        self._cache_stats["vector_misses"] = int(self._cache_stats.get("vector_misses", 0)) + 1

        start_time = datetime.utcnow()
        if not milvus_client.connected:
            milvus_client.connect()
        try:
            collection = milvus_client.create_collection_if_not_exists(
                collection_name, dimension=384, load=False
            )
        except Exception as e:
            logger.warning(
                "Milvus not available for vector search, fallback to empty results: %s",
                e,
            )
            return []
        try:
            from pymilvus import utility
            if not utility.has_collection(collection_name):
                logger.warning(f"Collection '{collection_name}' does not exist")
                return []
        except Exception as e:
            logger.warning("Failed to check collection existence: %s", e)
            return []
        if collection_name not in self._milvus_loaded_collections:
            try:
                from pymilvus import utility
                load_state = utility.load_state(collection_name)

                is_loaded = False
                if isinstance(load_state, dict):
                    state_value = (
                        load_state.get("state", "").upper()
                        if isinstance(load_state.get("state"), str)
                        else str(load_state.get("state", "")).upper()
                    )
                    is_loaded = state_value in ("LOADED", "LOADED_FOR_SEARCH")
                elif isinstance(load_state, str):
                    is_loaded = load_state.upper() in ("LOADED", "LOADED_FOR_SEARCH")
                else:
                    is_loaded = "LOADED" in str(load_state).upper()

                if not is_loaded:
                    logger.info(
                        "Collection '%s' is not loaded (state: %s), loading now...",
                        collection_name,
                        load_state,
                    )
                    collection.load()
                self._milvus_loaded_collections.add(collection_name)
            except Exception as e:
                logger.warning(
                    "Failed to ensure collection '%s' loaded, will rely on retry: %s",
                    collection_name,
                    e,
                )
        query_vector = [self.generate_embedding(query)]
        try:
            search_params = {
                "metric_type": str(MILVUS_METRIC_TYPE or "L2"),
                "params": {"nprobe": int(MILVUS_NPROBE or 10)},
            }
            results = collection.search(
                data=query_vector,
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                output_fields=["text_id"]
            )
        except Exception as e:
            if "not loaded" in str(e).lower() or "collection not loaded" in str(e).lower():
                logger.warning(
                    "Search failed due to collection not loaded, retrying after reload: %s",
                    e,
                )
                try:
                    collection.load()
                    self._milvus_loaded_collections.add(collection_name)
                    results = collection.search(
                        data=query_vector,
                        anns_field="embedding",
                        param=search_params,
                        limit=top_k,
                        output_fields=["text_id"]
                    )
                except Exception as retry_error:
                    logger.error("Retry search failed: %s", retry_error)
                    return []
            else:
                logger.error("Search failed: %s", e)
                return []
        search_results = []
        if results and len(results) > 0:
            for hit in results[0]:
                search_results.append({
                    "id": hit.id,
                    "text_id": hit.entity.get("text_id", ""),
                    "distance": hit.distance,
                    "score": 1 / (1 + hit.distance) if hit.distance > 0 else 1.0,
                })
        
        elapsed_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
        logger.info(
            "vector_search done: collection=%s, top_k=%d, hits=%d, elapsed=%.1fms",
            collection_name,
            top_k,
            len(search_results),
            elapsed_ms,
        )

        self._cache_set_vector(cache_key, [dict(item) for item in search_results])
        self._log_cache_stats_if_needed()

        return search_results
    
    async def graph_search(self, entity_name: str, relation_type: str = None, limit: int = 10) -> List[Dict[str, Any]]:
        """å›¾æ•°æ®åº“å…³ç³»æŸ¥è¯¢ã€‚relation_type ç™½åå•æ ¡éªŒåæ‹¼æ¥ï¼Œé¿å…æ³¨å…¥ï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰ã€‚"""
        rel = None
        if relation_type and isinstance(relation_type, str):
            rel_candidate = relation_type.strip().upper()
            if re.match(r"^[A-Z_][A-Z0-9_]*$", rel_candidate):
                rel = rel_candidate

        if rel:
            query = f"""
            MATCH (a)-[r:{rel}]->(b)
            WHERE a.name CONTAINS $name OR b.name CONTAINS $name
            RETURN a, r, b, labels(a) as a_labels, labels(b) as b_labels, type(r) as rel_type
            LIMIT $limit
            """
        else:
            query = """
            MATCH (a)-[r]->(b)
            WHERE a.name CONTAINS $name OR b.name CONTAINS $name
            RETURN a, r, b, labels(a) as a_labels, labels(b) as b_labels, type(r) as rel_type
            LIMIT $limit
            """
        
        loop = asyncio.get_event_loop()
        results = await loop.run_in_executor(
            None,
            neo4j_client.execute_query,
            query,
            {"name": entity_name, "limit": limit}
        )
        
        return results or []

    async def _graph_search_many(
        self, entity_names: List[str], relation_type: str = None, per_entity_limit: int = 5
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡å›¾å…³ç³»æŸ¥è¯¢ï¼šæŠŠå¤šæ¬¡ graph_search åˆå¹¶ä¸ºä¸€æ¬¡ Neo4j æŸ¥è¯¢ï¼Œå‡å°‘ round-tripã€‚"""
        names = [str(x).strip() for x in (entity_names or []) if str(x).strip()]
        if not names:
            return []
        names = names[:10]

        rel = None
        if relation_type and isinstance(relation_type, str):
            rel_candidate = relation_type.strip().upper()
            if re.match(r"^[A-Z_][A-Z0-9_]*$", rel_candidate):
                rel = rel_candidate

        per_limit = max(1, min(int(per_entity_limit or 5), 20))
        if rel:
            query = f"""
            UNWIND $names AS name
            CALL {{
              WITH name
              MATCH (a)-[r:{rel}]->(b)
              WHERE a.name CONTAINS name OR b.name CONTAINS name
              RETURN a, r, b, labels(a) as a_labels, labels(b) as b_labels, type(r) as rel_type
              LIMIT $per_limit
            }}
            RETURN name as query_name, a, r, b, a_labels, b_labels, rel_type
            """
        else:
            query = """
            UNWIND $names AS name
            CALL {
              WITH name
              MATCH (a)-[r]->(b)
              WHERE a.name CONTAINS name OR b.name CONTAINS name
              RETURN a, r, b, labels(a) as a_labels, labels(b) as b_labels, type(r) as rel_type
              LIMIT $per_limit
            }
            RETURN name as query_name, a, r, b, a_labels, b_labels, rel_type
            """

        loop = asyncio.get_event_loop()
        try:
            rows = await loop.run_in_executor(
                None,
                neo4j_client.execute_query,
                query,
                {"names": names, "per_limit": per_limit},
            )
            return rows or []
        except Exception as e:
            logger.warning("_graph_search_many failed: %s", e)
            return []
    
    async def graph_subgraph_search(self, entities: List[str], depth: int = 2) -> Dict[str, Any]:
        """åŸºäºå¤šå®ä½“æ„å»ºå­å›¾ã€‚depth ç»æ ¡éªŒåæ‹¼æ¥ï¼ˆNeo4j é™åˆ¶ï¼‰ã€‚"""
        if not entities:
            return {"nodes": [], "relationships": []}
        safe_depth = 2
        try:
            safe_depth = int(depth)
        except Exception:
            safe_depth = 2
        safe_depth = max(1, min(safe_depth, 3))

        query = f"""
        MATCH path = (a)-[*1..{safe_depth}]-(b)
        WHERE a.name IN $entities OR b.name IN $entities
        WITH path, nodes(path) as nodes_list, relationships(path) as rels_list
        UNWIND nodes_list as node
        UNWIND rels_list as rel
        RETURN DISTINCT 
            id(node) as node_id,
            labels(node) as labels,
            properties(node) as properties,
            id(rel) as rel_id,
            type(rel) as rel_type,
            properties(rel) as rel_properties
        LIMIT 50
        """
        loop = asyncio.get_event_loop()
        results = await loop.run_in_executor(
            None,
            neo4j_client.execute_query,
            query,
            {"entities": entities}
        )
        nodes = {}
        relationships = []
        
        for record in results:
            if 'node_id' in record:
                node_id = record['node_id']
                if node_id not in nodes:
                    nodes[node_id] = {
                        "id": node_id,
                        "labels": record.get('labels', []),
                        "properties": record.get('properties', {})
                    }
            
            if 'rel_id' in record and record['rel_id']:
                relationships.append({
                    "id": record['rel_id'],
                    "type": record.get('rel_type'),
                    "properties": record.get('rel_properties', {})
                })
        
        return {
            "nodes": list(nodes.values()),
            "relationships": relationships,
            "entity_count": len(entities)
        }
    
    def _query_needs_context(self, query: str) -> bool:
        """å¯’æš„/è‡´è°¢/å‘Šåˆ«/èƒ½åŠ›è¯¢é—®ç­‰è¿”å› Falseï¼Œä¸æ£€ç´¢ï¼›æ™¯åŒº/æ™¯ç‚¹é—®é¢˜æ‰èµ° RAGã€‚"""
        if not query or not isinstance(query, str):
            return False
        q = query.strip()
        if len(q) <= 1:
            return False
        no_context_patterns = [
            r"^(ä½ å¥½|æ‚¨å¥½|å—¨|hello|hi|åœ¨å—|åœ¨ä¸åœ¨)\s*[ï¼Ÿ?]?$",
            r"^(è°¢è°¢|æ„Ÿè°¢|å¤šè°¢|è°¢è°¢æ‚¨)\s*[ï¼!ã€‚.]?$",
            r"^(å†è§|æ‹œæ‹œ|bye)\s*[ï¼!ã€‚.]?$",
            r"^(ä½ æ˜¯è°|ä½ èƒ½åšä»€ä¹ˆ|æœ‰ä»€ä¹ˆåŠŸèƒ½|ä½ èƒ½å¹²å˜›|ä»‹ç»ä¸‹è‡ªå·±)\s*[ï¼Ÿ?]?$",
            r"^(å¸®åŠ©|help|æ€ä¹ˆç”¨|å¦‚ä½•ä½¿ç”¨)\s*[ï¼Ÿ?]?$",
            r"^éšä¾¿(é—®é—®|é—®é—®çœ‹)?\s*[ï¼Ÿ?]?$",
        ]
        for pat in no_context_patterns:
            if re.search(pat, q, re.IGNORECASE):
                return False
        return True
    
    def _is_listing_query(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºâ€œæ™¯ç‚¹åˆ—è¡¨/æ•°é‡â€ç±»é—®é¢˜ï¼Œä¾‹å¦‚æœ‰å“ªäº›æ™¯ç‚¹ã€æ™¯ç‚¹åˆ†å¸ƒã€å¤šå°‘ä¸ªæ™¯ç‚¹ç­‰ã€‚"""
        if not query or not isinstance(query, str):
            return False
        q = query.strip()
        if not q:
            return False
        pattern = (
            r"æœ‰å“ªäº›æ™¯ç‚¹|æ™¯ç‚¹éƒ½æœ‰(ä»€ä¹ˆ|å“ªäº›)|æ™¯ç‚¹æƒ…å†µ|æ™¯ç‚¹åˆ†å¸ƒ|æœ‰ä»€ä¹ˆæ™¯ç‚¹|æ™¯ç‚¹.*æœ‰å“ªäº›"
            r"|æœ‰å¤šå°‘ä¸ª?æ™¯ç‚¹|å¤šå°‘ä¸ªæ™¯ç‚¹|æ™¯ç‚¹æœ‰å¤šå°‘ä¸ª?|æ™¯åŒºæœ‰å¤šå°‘ä¸ª?æ™¯ç‚¹|å‡ ä¸ªæ™¯ç‚¹"
        )
        return bool(re.search(pattern, q))

    def _is_route_query(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºâ€œè·¯çº¿/è¡Œç¨‹/æ¨èè·¯çº¿â€ç±»é—®é¢˜ï¼Œéœ€è¦å¤šæ™¯ç‚¹ä¸²è”å›ç­”ã€‚"""
        if not query or not isinstance(query, str):
            return False
        q = query.strip()
        if not q:
            return False
        pattern = (
            r"è·¯çº¿|è¡Œç¨‹|æ¨è.*(è·¯çº¿|æ€ä¹ˆèµ°|æ¸¸ç©é¡ºåº)|(äº²å­|ä¸€æ—¥æ¸¸|åŠæ—¥|æ¸¸è§ˆ).*è·¯çº¿"
            r"|æ€ä¹ˆèµ°|æ¸¸ç©è·¯çº¿|æ¸¸è§ˆè·¯çº¿|é€›.*é¡ºåº|å…ˆå».*å†å»|è·¯çº¿æ¨è|èµ°æ³•"
        )
        return bool(re.search(pattern, q))

    def _classify_query_intent(self, query: str) -> QueryIntent:
        """æ™ºèƒ½åˆ†ç±»æŸ¥è¯¢æ„å›¾ï¼Œè¿”å›å¯¹åº”çš„æ£€ç´¢ç­–ç•¥ç±»å‹ã€‚"""
        if not query or not isinstance(query, str):
            return QueryIntent.GENERAL
        q = query.strip().lower()
        if not q:
            return QueryIntent.GENERAL
        
        # è·¯çº¿/è¡Œç¨‹ç±»ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼Œå› ä¸ºéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
        if re.search(
            r"è·¯çº¿|è¡Œç¨‹|æ¨è.*(è·¯çº¿|æ€ä¹ˆèµ°|æ¸¸ç©é¡ºåº)|(äº²å­|ä¸€æ—¥æ¸¸|åŠæ—¥|æ¸¸è§ˆ).*è·¯çº¿"
            r"|æ€ä¹ˆèµ°|æ¸¸ç©è·¯çº¿|æ¸¸è§ˆè·¯çº¿|é€›.*é¡ºåº|å…ˆå».*å†å»|è·¯çº¿æ¨è|èµ°æ³•|æ¸¸è§ˆé¡ºåº",
            q
        ):
            return QueryIntent.ROUTE
        
        # åˆ—è¡¨/æ•°é‡ç±»
        if re.search(
            r"æœ‰å“ªäº›|éƒ½æœ‰(ä»€ä¹ˆ|å“ªäº›)|æƒ…å†µ|åˆ†å¸ƒ|æœ‰ä»€ä¹ˆ|.*æœ‰å“ªäº›"
            r"|æœ‰å¤šå°‘ä¸ª?|å¤šå°‘ä¸ª|æœ‰å¤šå°‘|å‡ ä¸ª|åˆ—ä¸¾|åˆ—å‡º",
            q
        ):
            return QueryIntent.LISTING
        
        # æ¯”è¾ƒç±»
        if re.search(
            r"å“ªä¸ª(æ›´å¥½|æ›´|æ¯”è¾ƒ|åŒºåˆ«|ä¸åŒ)|å¯¹æ¯”|æ¯”è¾ƒ|åŒºåˆ«|å·®å¼‚|å“ªä¸ªå¥½|å“ªä¸ªæ›´",
            q
        ):
            return QueryIntent.COMPARISON
        
        # ä½ç½®/å¯¼èˆªç±»
        if re.search(
            r"åœ¨å“ª|ä½ç½®|åœ°å€|æ€ä¹ˆå»|æ€ä¹ˆåˆ°|å¯¼èˆª|è·ç¦»|å¤šè¿œ|é™„è¿‘|å‘¨å›´",
            q
        ):
            return QueryIntent.LOCATION
        
        # ç‰¹è‰²/åŠŸèƒ½ç±»
        if re.search(
            r"ç‰¹è‰²|ç‰¹ç‚¹|å¥½ç©|æœ‰ä»€ä¹ˆå¥½ç©çš„|æœ‰ä»€ä¹ˆ|åŠŸèƒ½|äº®ç‚¹|æ¨èç†ç”±|ä¸ºä»€ä¹ˆ|å€¼å¾—",
            q
        ):
            return QueryIntent.FEATURE
        
        # è¯¦æƒ…/ä»‹ç»ç±»ï¼ˆåŒ…å«å…·ä½“æ™¯ç‚¹åæˆ–"ä»‹ç»"ï¼‰
        if re.search(
            r"ä»‹ç»|è¯¦æƒ…|è¯¦ç»†|æ˜¯ä»€ä¹ˆ|ä»€ä¹ˆæ ·|æè¿°|è¯´è¯´|è®²è®²|äº†è§£",
            q
        ):
            return QueryIntent.DETAIL
        
        return QueryIntent.GENERAL

    def _get_search_strategy(self, intent: QueryIntent) -> Dict[str, Any]:
        """æ ¹æ®æ„å›¾è¿”å›æ£€ç´¢ç­–ç•¥é…ç½®ï¼ˆtop_k, é˜ˆå€¼, å›¾æŸ¥è¯¢æ·±åº¦ç­‰ï¼‰ã€‚"""
        strategies = {
            QueryIntent.ROUTE: {
                "top_k": 10,  # è·¯çº¿éœ€è¦æ›´å¤šå€™é€‰
                "relevance_threshold": 0.1,  # é™ä½é˜ˆå€¼ï¼Œå…è®¸æ›´å¤šç›¸å…³ç»“æœ
                "graph_depth": 3,  # æ·±åº¦å›¾æŸ¥è¯¢ï¼Œæ‰¾æ›´å¤šå…³è”
                "expand_scenic_attractions": True,  # æ‰©å±•åŒæ™¯åŒºå¤šæ™¯ç‚¹
                "max_attractions": 15,  # æœ€å¤š15ä¸ªæ™¯ç‚¹ä¾›è·¯çº¿ä¸²è”
                "force_at_least_one": True,  # å³ä½¿ä½åˆ†ä¹Ÿä¿ç•™è‡³å°‘ä¸€ä¸ª
            },
            QueryIntent.LISTING: {
                "top_k": 8,
                "relevance_threshold": 0.15,
                "graph_depth": 2,
                "expand_scenic_attractions": True,
                "max_attractions": 30,  # åˆ—è¡¨éœ€è¦æ›´å¤šæ™¯ç‚¹
                "force_at_least_one": True,
            },
            QueryIntent.DETAIL: {
                "top_k": 3,  # è¯¦æƒ…æŸ¥è¯¢ç²¾å‡†å³å¯
                "relevance_threshold": 0.3,  # æé«˜é˜ˆå€¼ï¼Œåªè¦é«˜ç›¸å…³
                "graph_depth": 1,  # æµ…æŸ¥è¯¢ï¼ŒåªæŸ¥ç›´æ¥å…³ç³»
                "expand_scenic_attractions": False,  # ä¸æ‰©å±•ï¼Œä¸“æ³¨å•ç‚¹
                "max_attractions": 1,
                "force_at_least_one": False,
            },
            QueryIntent.COMPARISON: {
                "top_k": 8,  # æ¯”è¾ƒéœ€è¦å¤šä¸ªå®ä½“
                "relevance_threshold": 0.2,
                "graph_depth": 2,
                "expand_scenic_attractions": False,
                "max_attractions": 5,  # æ¯”è¾ƒç±»é™åˆ¶æ•°é‡
                "force_at_least_one": True,
            },
            QueryIntent.LOCATION: {
                "top_k": 5,
                "relevance_threshold": 0.2,
                "graph_depth": 2,  # æŸ¥ä½ç½®å…³ç³»
                "expand_scenic_attractions": False,
                "max_attractions": 1,
                "force_at_least_one": True,
            },
            QueryIntent.FEATURE: {
                "top_k": 6,
                "relevance_threshold": 0.2,
                "graph_depth": 2,  # æŸ¥ç‰¹è‰²/å±æ€§å…³ç³»
                "expand_scenic_attractions": False,
                "max_attractions": 3,
                "force_at_least_one": True,
            },
            QueryIntent.GENERAL: {
                "top_k": 5,  # é»˜è®¤å€¼
                "relevance_threshold": RAG_RELEVANCE_SCORE_THRESHOLD,
                "graph_depth": 2,
                "expand_scenic_attractions": False,
                "max_attractions": 1,
                "force_at_least_one": True,
            },
        }
        return strategies.get(intent, strategies[QueryIntent.GENERAL])

    async def _get_scenic_spot_by_attraction_id(self, attraction_id: int) -> Optional[Dict[str, Any]]:
        """é€šè¿‡æ™¯ç‚¹ id åæŸ¥æ‰€å±æ™¯åŒºï¼Œè¿”å› {'sid', 's_name'} æˆ– Noneï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰ã€‚"""
        try:
            query = """
            MATCH (a:Attraction {id: $aid})
            OPTIONAL MATCH (a)-[:å±äº]->(s1:ScenicSpot)
            OPTIONAL MATCH (s2:ScenicSpot)-[:HAS_SPOT]->(a)
            WITH a, coalesce(s1, s2) AS s WHERE s IS NOT NULL
            RETURN s.scenic_spot_id AS sid, s.name AS s_name
            LIMIT 1
            """
            loop = asyncio.get_event_loop()
            rows = await loop.run_in_executor(
                None,
                neo4j_client.execute_query,
                query,
                {"aid": int(attraction_id)}
            )
            if rows:
                row0 = rows[0]
                return {
                    "sid": row0.get("sid"),
                    "s_name": row0.get("s_name"),
                }
        except Exception as e:
            logger.warning(f"_get_scenic_spot_by_attraction_id failed attraction_id={attraction_id}: {e}")
        return None

    async def _get_scenic_attractions_sentence_by_name(self, scenic_name: str) -> str:
        """æ ¹æ®æ™¯åŒºåç§°æŸ¥è¯¢å…¶ä¸‹ç›¸å…³æ™¯ç‚¹ï¼Œå¹¶æ ¼å¼åŒ–ä¸ºä¸€å¥è¯æè¿°ï¼ˆå¸¦æ•°é‡ä¿¡æ¯ï¼Œå¼‚æ­¥ï¼‰ã€‚"""
        scenic_name = (scenic_name or "").strip()
        if not scenic_name:
            return ""
        try:
            scenic_attractions_q = """
            MATCH (s:ScenicSpot {name: $name})
            OPTIONAL MATCH (s)-[:HAS_SPOT]->(n)
            OPTIONAL MATCH (s)<-[:å±äº]-(a:Attraction)
            WITH s, collect(DISTINCT n) AS spot_list, collect(DISTINCT a) AS att_list
            UNWIND spot_list + att_list AS x
            WITH s, x WHERE x IS NOT NULL
            WITH DISTINCT s, x, coalesce(x.name, x.text_id) AS xname
            WHERE xname IS NOT NULL AND NOT (xname STARTS WITH 'kb_')
            RETURN s.name AS scenic_name, xname AS attraction_name
            ORDER BY attraction_name
            LIMIT 50
            """
            loop = asyncio.get_event_loop()
            rows = await loop.run_in_executor(
                None,
                neo4j_client.execute_query,
                scenic_attractions_q,
                {"name": scenic_name}
            ) or []
        except Exception as e:
            logger.warning(f"_get_scenic_attractions_sentence_by_name query failed scenic_name={scenic_name}: {e}")
            return ""

        attraction_names: List[str] = []
        for row in rows:
            nm = row.get("attraction_name")
            if not nm or nm in attraction_names:
                continue
            attraction_names.append(nm)
        if not attraction_names:
            return ""
        count = len(attraction_names)
        joined = "ã€".join(attraction_names)
        return f"æ ¹æ®å›¾æ•°æ®åº“ï¼Œæ™¯åŒºã€Œ{scenic_name}ã€ä¸‹çš„ç›¸å…³æ™¯ç‚¹å…±æœ‰ {count} ä¸ªï¼ŒåŒ…æ‹¬ï¼š{joined}ã€‚"

    async def hybrid_search(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """
        æ„å›¾é©±åŠ¨çš„æ··åˆæ£€ç´¢ï¼šå‘é‡æ£€ç´¢ + å®ä½“è¯†åˆ« + å›¾æ£€ç´¢ + ç»“æœèåˆã€‚
        æ ¹æ®æŸ¥è¯¢æ„å›¾è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ£€ç´¢ç­–ç•¥ï¼ˆtop_kã€é˜ˆå€¼ã€å›¾æŸ¥è¯¢æ·±åº¦ç­‰ï¼‰ã€‚
        """
        # 1. æ„å›¾åˆ†ç±»
        intent = self._classify_query_intent(query)
        strategy = self._get_search_strategy(intent)
        
        # ä½¿ç”¨ç­–ç•¥ä¸­çš„ top_kï¼ˆå¦‚æœå¤–éƒ¨ä¼ å…¥çš„ top_k ä¸æ˜¯é»˜è®¤å€¼ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨å€¼ï¼‰
        effective_top_k = top_k if top_k != 5 else strategy["top_k"]
        effective_threshold = strategy["relevance_threshold"]
        graph_depth = strategy["graph_depth"]
        
        logger.debug(f"æŸ¥è¯¢æ„å›¾: {intent.value}, top_k={effective_top_k}, threshold={effective_threshold}, graph_depth={graph_depth}")
        
        errors: Dict[str, str] = {}
        try:
            vector_results = await self.vector_search(query, top_k=effective_top_k)
        except Exception as e:
            errors["milvus"] = str(e)
            logger.warning("hybrid_search vector_search failed (fallback to empty): %s", e)
            vector_results = []
        
        # ä½¿ç”¨ç­–ç•¥ä¸­çš„é˜ˆå€¼è¿‡æ»¤
        vector_results_relevant = [
            r
            for r in (vector_results or [])
            if (r.get("score") or 0) >= effective_threshold
        ]
        # æ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦å¼ºåˆ¶ä¿ç•™è‡³å°‘ä¸€ä¸ªç»“æœ
        if not vector_results_relevant and vector_results and strategy.get("force_at_least_one", True):
            vector_results_relevant = vector_results[:1]
        vector_results = vector_results_relevant

        texts_to_extract = [query]
        if vector_results:
            text_ids = [
                r.get("text_id", "")
                for r in vector_results[:3]
                if r.get("text_id")
                and not (r.get("text_id", "").strip().startswith("kb_") or r.get("text_id", "").strip().startswith("attraction_"))
            ]
            texts_to_extract.extend(text_ids)
        
        if len(texts_to_extract) > 1:
            loop = asyncio.get_event_loop()
            entities_list = await asyncio.gather(*[
                loop.run_in_executor(None, self.extract_entities, text)
                for text in texts_to_extract
            ])
            entities = []
            for ent_list in entities_list:
                entities.extend(ent_list)
        else:
            entities = self.extract_entities(query)
        
        unique_entities = {}
        for entity in entities:
            text = entity["text"]
            if text not in unique_entities or entity["confidence"] > unique_entities[text]["confidence"]:
                unique_entities[text] = entity
        
        entity_names = [e["text"] for e in unique_entities.values()]
        
        text_ids_to_fetch = [
            (r.get("text_id") or "").strip()
            for r in (vector_results or [])
            if (r.get("text_id") or "").strip() and not (r.get("text_id") or "").strip().startswith("attraction_")
        ]
        
        graph_results: List[Dict[str, Any]] = []
        subgraph_data = None
        if entity_names:
            # æ ¹æ®æ„å›¾è°ƒæ•´å›¾æŸ¥è¯¢å‚æ•°
            per_entity_limit = 8 if intent == QueryIntent.ROUTE else 5
            tasks = [
                self._graph_search_many(entity_names[:5], per_entity_limit=per_entity_limit),
            ]
            # æ ¹æ®ç­–ç•¥ä¸­çš„ graph_depth å†³å®šæ˜¯å¦è¿›è¡Œå­å›¾æŸ¥è¯¢
            if len(entity_names) > 1 and graph_depth > 1:
                tasks.append(self.graph_subgraph_search(entity_names[:3], depth=graph_depth))
            results = await asyncio.gather(*tasks, return_exceptions=True)
            if results:
                r0 = results[0]
                if not isinstance(r0, Exception):
                    graph_results = r0 or []
                elif isinstance(r0, Exception):
                    errors["neo4j_graph"] = str(r0)
            if len(tasks) > 1 and len(results) > 1:
                r1 = results[1]
                if not isinstance(r1, Exception):
                    subgraph_data = r1
                else:
                    errors["neo4j_subgraph"] = str(r1)
        text_contents = {}
        if text_ids_to_fetch:
            loop = asyncio.get_event_loop()
            try:
                text_contents = await loop.run_in_executor(
                    None,
                    self._get_text_contents_from_neo4j,
                    text_ids_to_fetch,
                )
            except Exception as e:
                errors["neo4j_text"] = str(e)
                logger.warning("hybrid_search fetch text contents failed: %s", e)
                text_contents = {}
        
        for r in vector_results or []:
            tid = (r.get("text_id") or "").strip()
            if tid and tid in text_contents:
                r["content"] = text_contents[tid]
        enhanced_results = self._merge_results(vector_results, graph_results, entity_names)
        attraction_ids = []
        primary_attraction_id = None
        for r in (vector_results or []):
            text_id = (r.get("text_id") or "").strip()
            if text_id.startswith("attraction_"):
                try:
                    aid = int(text_id.replace("attraction_", ""))
                    attraction_ids.append(aid)
                    if primary_attraction_id is None:
                        primary_attraction_id = aid
                except ValueError:
                    pass
        
        # æ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦æ‰©å±•åŒæ™¯åŒºå¤šæ™¯ç‚¹
        should_expand = strategy.get("expand_scenic_attractions", False)
        max_attractions = strategy.get("max_attractions", 1)
        
        if should_expand and primary_attraction_id is not None:
            try:
                parent_info = await self._get_scenic_spot_by_attraction_id(primary_attraction_id)
                if parent_info:
                    s_name = parent_info.get("s_name")
                    if s_name:
                        # åˆå¹¶æŸ¥è¯¢ï¼šä¸€æ¬¡è·å–æ™¯ç‚¹åˆ—è¡¨å’Œ IDï¼Œé¿å…ä¸¤æ¬¡ Neo4j å¾€è¿”
                        async def fetch_scenic_attractions():
                            scenic_aids_q = """
                            MATCH (s:ScenicSpot {name: $name})
                            OPTIONAL MATCH (s)<-[:å±äº]-(a:Attraction)
                            OPTIONAL MATCH (s)-[:HAS_SPOT]->(a2:Attraction)
                            WITH collect(DISTINCT a) + collect(DISTINCT a2) AS xs
                            UNWIND xs AS x
                            WITH DISTINCT x WHERE x IS NOT NULL AND x.id IS NOT NULL
                            RETURN x.id AS aid, x.name AS name
                            ORDER BY aid
                            LIMIT 200
                            """
                            loop = asyncio.get_event_loop()
                            rows = await loop.run_in_executor(
                                None,
                                neo4j_client.execute_query,
                                scenic_aids_q,
                                {"name": str(s_name).strip()}
                            ) or []
                            aids: List[int] = []
                            names: List[str] = []
                            for rr in rows:
                                if rr and rr.get("aid") is not None:
                                    try:
                                        aids.append(int(rr["aid"]))
                                        if rr.get("name"):
                                            names.append(str(rr["name"]))
                                    except Exception:
                                        continue
                            return aids, names
                        
                        scenic_aids, attraction_names = await fetch_scenic_attractions()
                        
                        # ç”Ÿæˆå¥å­æè¿°ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                        if attraction_names and "æ ¹æ®å›¾æ•°æ®åº“ï¼Œæ™¯åŒºã€Œ" not in (enhanced_results or ""):
                            count = len(attraction_names)
                            joined = "ã€".join(attraction_names[:20])  # æœ€å¤šæ˜¾ç¤º20ä¸ª
                            sentence = f"æ ¹æ®å›¾æ•°æ®åº“ï¼Œæ™¯åŒºã€Œ{s_name}ã€ä¸‹çš„ç›¸å…³æ™¯ç‚¹å…±æœ‰ {count} ä¸ªï¼ŒåŒ…æ‹¬ï¼š{joined}ã€‚"
                            enhanced_results = sentence + "\n\n" + (enhanced_results or "")
                        
                        if scenic_aids:
                            # ä½¿ç”¨ç­–ç•¥ä¸­çš„ max_attractions
                            clusters_ctx = await self._get_attraction_cluster_context(scenic_aids, max_items=max_attractions)
                            if clusters_ctx:
                                # æ ¹æ®æ„å›¾æ·»åŠ ä¸åŒçš„æ ‡é¢˜
                                if intent == QueryIntent.ROUTE:
                                    enhanced_results = (enhanced_results or "") + "\n\nã€è·¯çº¿å¯é€‰æ™¯ç‚¹ã€‘\n" + clusters_ctx
                                else:
                                    enhanced_results = (enhanced_results or "") + "\n\n" + clusters_ctx
            except Exception as e:
                logger.warning(f"æ‰©å±•æ™¯åŒºæ™¯ç‚¹å¤±è´¥ (intent={intent.value}): {e}")
        # åˆ—ä¸¾ç±»é—®é¢˜ï¼ˆå¦‚ã€Œè¿™ä¸ªæ™¯åŒºæœ‰å¤šå°‘æ™¯ç‚¹ã€ï¼‰è‹¥å‘é‡æœªå‘½ä¸­ attraction_XXï¼Œåˆ™æ—  primary_attraction_idï¼Œ
        # æ­¤å¤„å…œåº•ï¼šä»å›¾åº“æŸ¥æ‰€æœ‰æ™¯åŒºï¼Œè¡¥å……è‡³å°‘ä¸€ä¸ªæ™¯åŒºçš„æ™¯ç‚¹æ•°é‡ï¼Œé¿å…ã€ŒæŸ¥ä¸åˆ°ã€ã€‚
        if intent == QueryIntent.LISTING and "æ ¹æ®å›¾æ•°æ®åº“ï¼Œæ™¯åŒºã€Œ" not in (enhanced_results or ""):
            try:
                async def fetch_first_scenic_listing():
                    all_scenic_q = """
                    MATCH (s:ScenicSpot) RETURN s.name AS name LIMIT 5
                    """
                    loop = asyncio.get_event_loop()
                    rows = await loop.run_in_executor(
                        None,
                        neo4j_client.execute_query,
                        all_scenic_q,
                        {}
                    ) or []
                    for row in rows:
                        nm = (row.get("name") or "").strip() if row else ""
                        if not nm:
                            continue
                        sentence = await self._get_scenic_attractions_sentence_by_name(nm)
                        if sentence:
                            return sentence
                    return None
                
                sentence = await fetch_first_scenic_listing()
                if sentence:
                    enhanced_results = (sentence + "\n\n" + (enhanced_results or "")).strip()
            except Exception as e:
                logger.warning(f"åˆ—ä¸¾æŸ¥è¯¢å…œåº•æŸ¥æ™¯åŒºæ™¯ç‚¹æ•°é‡å¤±è´¥: {e}")
        query_about_scenic = bool(re.search(r"ä»€ä¹ˆæ™¯åŒº|å“ªä¸ªæ™¯åŒº|æ˜¯å•¥æ™¯åŒº|è¿™æ˜¯ä»€ä¹ˆæ™¯åŒº|æ˜¯å“ªä¸ªæ™¯åŒº|å•¥æ™¯åŒº|å“ªä¸ªæ™¯ç‚¹.*æ™¯åŒº|ä»‹ç».*æ™¯åŒº|æ™¯åŒº.*ä»‹ç»|è¿™ä¸ªæ™¯åŒº", (query or "").strip()))
        scenic_ctx_found = False
        if query_about_scenic:
            scenic_tasks = []
            if primary_attraction_id is not None:
                async def get_scenic_from_attraction():
                    try:
                        parent_info = await self._get_scenic_spot_by_attraction_id(primary_attraction_id)
                        if parent_info:
                            sid = parent_info.get("sid")
                            s_name = parent_info.get("s_name")
                            if sid is not None:
                                return await self._get_scenic_spot_cluster_context(int(sid))
                            if s_name:
                                return await self._get_scenic_spot_cluster_context_by_name(str(s_name).strip())
                    except Exception as e:
                        logger.warning(f"æŸ¥è¯¢æ™¯ç‚¹æ‰€å±æ™¯åŒºå¤±è´¥: {e}")
                    return ""
                scenic_tasks.append(get_scenic_from_attraction())
            
            if entity_names:
                async def get_scenic_from_entity(name):
                    try:
                        scenic_check_q = """
                        MATCH (s:ScenicSpot {name: $name})
                        RETURN s.scenic_spot_id AS sid, s.name AS s_name
                        LIMIT 1
                        """
                        loop = asyncio.get_event_loop()
                        scenic_rows = await loop.run_in_executor(
                            None,
                            neo4j_client.execute_query,
                            scenic_check_q,
                            {"name": name}
                        )
                        if scenic_rows:
                            row0 = scenic_rows[0]
                            sid = row0.get("sid")
                            s_name = row0.get("s_name")
                            if sid is not None:
                                return await self._get_scenic_spot_cluster_context(int(sid))
                            if s_name:
                                return await self._get_scenic_spot_cluster_context_by_name(str(s_name).strip())
                    except Exception as e:
                        logger.warning(f"ä»å®ä½“åç§°æŸ¥æ‰¾æ™¯åŒºå¤±è´¥: {e}")
                    return ""
                for entity_name in entity_names[:3]:
                    scenic_tasks.append(get_scenic_from_entity(entity_name))
            
            if subgraph_data:
                async def get_scenic_from_subgraph():
                    for node in subgraph_data.get("nodes", []):
                        labels = set(node.get("labels") or [])
                        props = node.get("properties") or {}
                        if "ScenicSpot" in labels and isinstance(props.get("name"), str):
                            scenic_name = props["name"]
                            try:
                                return await self._get_scenic_spot_cluster_context_by_name(scenic_name)
                            except Exception as e:
                                logger.warning(f"ä»å­å›¾æŸ¥æ‰¾æ™¯åŒºå¤±è´¥: {e}")
                                continue
                    return ""
                scenic_tasks.append(get_scenic_from_subgraph())
            
            if scenic_tasks:
                scenic_results = await asyncio.gather(*scenic_tasks, return_exceptions=True)
                for scenic_ctx in scenic_results:
                    if scenic_ctx and not isinstance(scenic_ctx, Exception) and scenic_ctx.strip():
                        enhanced_results = scenic_ctx + "\n\n" + (enhanced_results or "")
                        scenic_ctx_found = True
                        break
        # éæ‰©å±•ç±»æ„å›¾ä¸”æœªæ‰©å±•æ—¶ï¼Œæ·»åŠ å•æ™¯ç‚¹ç°‡ä¿¡æ¯
        if (not should_expand) and primary_attraction_id is not None and not (query_about_scenic and scenic_ctx_found):
            cluster_ctx = await self._get_attraction_cluster_context([primary_attraction_id], max_items=1)
            if cluster_ctx:
                enhanced_results = (enhanced_results or "") + "\n\n" + cluster_ctx
        
        return {
            "vector_results": vector_results,
            "graph_results": graph_results,
            "subgraph": subgraph_data,
            "entities": list(unique_entities.values()),
            "enhanced_context": enhanced_results,
            "query": query,
            "attraction_ids": attraction_ids,
            "primary_attraction_id": primary_attraction_id,
            "errors": errors,
            "intent": intent.value,  # è¿”å›æ„å›¾ç±»å‹ï¼Œä¾¿äºè°ƒè¯•
            "strategy": {k: v for k, v in strategy.items() if k not in ["expand_scenic_attractions"]},  # è¿”å›ç­–ç•¥ï¼ˆæ’é™¤å†…éƒ¨æ ‡å¿—ï¼‰
        }

    async def _build_scenic_attractions_context(
        self,
        query: str,
        rag_results: Dict[str, Any],
        conversation_history: Optional[List[Dict[str, str]]] = None,
    ) -> str:
        """åˆ—ä¸¾æ™¯ç‚¹ç±»é—®é¢˜æ—¶ï¼Œè¡¥å……â€œæŸæ™¯åŒºä¸‹æœ‰å“ªäº›æ™¯ç‚¹â€çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚"""
        if not self._is_listing_query(query):
            return ""

        scenic_names: set[str] = set()
        subgraph = rag_results.get("subgraph") or {}
        for node in subgraph.get("nodes", []):
            labels = set(node.get("labels") or [])
            props = node.get("properties") or {}
            if "ScenicSpot" in labels and isinstance(props.get("name"), str):
                scenic_names.add(props["name"])
        if not scenic_names and conversation_history:
            recent_user_text = "ã€‚".join(
                msg["content"]
                for msg in conversation_history[-6:]
                if msg.get("role") == "user" and isinstance(msg.get("content"), str)
            )
            if recent_user_text:
                for ent in self.extract_entities(recent_user_text):
                    cand = ent.get("text")
                    if not cand or not isinstance(cand, str):
                        continue
                    try:
                        check_q = """
                        MATCH (s:ScenicSpot {name: $name})
                        RETURN s.name AS name
                        LIMIT 1
                        """
                        loop = asyncio.get_event_loop()
                        res = await loop.run_in_executor(
                            None,
                            neo4j_client.execute_query,
                            check_q,
                            {"name": cand}
                        )
                        if res and isinstance(res, list) and res[0].get("name"):
                            scenic_names.add(res[0]["name"])
                    except Exception:
                        continue
        if not scenic_names:
            try:
                async def fetch_scenic_names():
                    all_scenic_q = """
                    MATCH (s:ScenicSpot) RETURN s.name AS name LIMIT 5
                    """
                    loop = asyncio.get_event_loop()
                    rows = await loop.run_in_executor(
                        None,
                        neo4j_client.execute_query,
                        all_scenic_q,
                        {}
                    ) or []
                    names = set()
                    for row in rows:
                        nm = row.get("name")
                        if nm and isinstance(nm, str):
                            names.add(nm)
                    return names
                scenic_names = await fetch_scenic_names()
            except Exception as e:
                logger.warning(f"query all ScenicSpot names failed: {e}")

        if not scenic_names:
            return ""
        parts: List[str] = []
        # å¹¶è¡ŒæŸ¥è¯¢å¤šä¸ªæ™¯åŒºçš„æ™¯ç‚¹åˆ—è¡¨
        tasks = [self._get_scenic_attractions_sentence_by_name(name) for name in list(scenic_names)[:3]]
        sentences = await asyncio.gather(*tasks, return_exceptions=True)
        for sentence in sentences:
            if sentence and not isinstance(sentence, Exception) and sentence.strip():
                parts.append(sentence)

        return "\n".join(parts)
    
    def _get_node_name(self, node: Any) -> str:
        """Neo4j èŠ‚ç‚¹å®‰å…¨å– nameã€‚"""
        if node is None:
            return ""
        if isinstance(node, dict):
            return (node.get("name") or (node.get("properties") or {}).get("name") or "").strip()
        if hasattr(node, "get"):
            return (node.get("name") or "").strip()
        return ""

    async def _get_attraction_cluster_context(self, attraction_ids: List[int], max_items: int = 20) -> str:
        """ä» Neo4j æ‹‰å–æ™¯ç‚¹ä¸€ç°‡ï¼ˆå±æ€§+å‡ºè¾¹ï¼‰ï¼Œæ ¼å¼åŒ–ä¸ºæ–‡æœ¬ä¾› LLMã€‚"""
        if not attraction_ids:
            return ""
        
        async def fetch_attraction_cluster(aid: int):
            try:
                query = """
                MATCH (a:Attraction {id: $id})
                OPTIONAL MATCH (a)-[r]->(n)
                RETURN a, type(r) as rel_type, n
                """
                loop = asyncio.get_event_loop()
                rows = await loop.run_in_executor(
                    None,
                    neo4j_client.execute_query,
                    query,
                    {"id": int(aid)}
                )
                if not rows:
                    return None
                att_name = ""
                att_desc = ""
                att_location = ""
                att_category = ""
                relations = []
                for row in rows:
                    a = row.get("a")
                    rel_type = row.get("rel_type")
                    n = row.get("n")
                    if a is not None and not att_name:
                        att_name = self._get_node_name(a) or (str(a.get("id")) if hasattr(a, "get") else "")
                        if hasattr(a, "get"):
                            att_desc = (a.get("description") or "").strip()
                            att_location = (a.get("location") or "").strip()
                            att_category = (a.get("category") or "").strip()
                        elif isinstance(a, dict):
                            att_desc = (a.get("description") or (a.get("properties") or {}).get("description") or "").strip()
                            att_location = (a.get("location") or (a.get("properties") or {}).get("location") or "").strip()
                            att_category = (a.get("category") or (a.get("properties") or {}).get("category") or "").strip()
                    if rel_type and n is not None:
                        n_name = self._get_node_name(n)
                        if n_name:
                            relations.append(f"{rel_type} -> {n_name}")
                if not att_name and not relations:
                    return None
                cluster_lines = [f"æ™¯ç‚¹ã€{att_name or ('ID:' + str(aid))}ã€‘"]
                if att_desc:
                    cluster_lines.append(f"æè¿°ï¼š{att_desc}")
                if att_location:
                    cluster_lines.append(f"ä½ç½®ï¼š{att_location}")
                if att_category:
                    cluster_lines.append(f"ç±»åˆ«ï¼š{att_category}")
                if relations:
                    cluster_lines.append("å…³ç³»ä¸å±æ€§ï¼š" + "ï¼›".join(relations))
                return "\n".join(cluster_lines)
            except Exception as e:
                logger.warning(f"æ‹‰å–æ™¯ç‚¹ç°‡å¤±è´¥ attraction_id={aid}: {e}")
                return None
        
        unique_ids: List[int] = []
        seen_ids: set[int] = set()
        for aid in attraction_ids:
            try:
                ia = int(aid)
            except Exception:
                continue
            if ia in seen_ids:
                continue
            seen_ids.add(ia)
            unique_ids.append(ia)
            if len(unique_ids) >= max(1, min(int(max_items), 80)):
                break
        results = await asyncio.gather(*[fetch_attraction_cluster(aid) for aid in unique_ids], return_exceptions=True)
        parts = [r for r in results if r and not isinstance(r, Exception)]
        
        if not parts:
            return ""
        return "ã€æ™¯ç‚¹ä¸€ç°‡ä¿¡æ¯ã€‘\n" + "\n\n".join(parts)

    def _parse_scenic_spot_rows(self, rows: List[Dict]) -> str:
        """è§£æ ScenicSpot è¡Œä¸ºæ™¯åŒºä¸€ç°‡æ–‡æœ¬ï¼Œä¾›æŒ‰ id/name å…±ç”¨ã€‚"""
        if not rows:
            return ""
        s_name = ""
        s_area = ""
        s_location = ""
        spot_names = []
        feature_names = []
        honor_names = []
        location_name = ""
        for row in rows or []:
            s = row.get("s")
            rel_type = row.get("rel_type")
            n = row.get("n")
            if s is not None and not s_name:
                if hasattr(s, "get"):
                    s_name = (s.get("name") or "").strip()
                    s_area = (s.get("area") or "").strip()
                    s_location = (s.get("location") or "").strip()
                elif isinstance(s, dict):
                    s_name = (s.get("name") or (s.get("properties") or {}).get("name") or "").strip()
                    s_area = (s.get("area") or (s.get("properties") or {}).get("area") or "").strip()
                    s_location = (s.get("location") or (s.get("properties") or {}).get("location") or "").strip()
            if rel_type and n is not None:
                n_name = self._get_node_name(n)
                if not n_name:
                    continue
                if rel_type == "HAS_SPOT":
                    spot_names.append(n_name)
                elif rel_type == "HAS_FEATURE":
                    feature_names.append(n_name)
                elif rel_type == "HAS_HONOR":
                    honor_names.append(n_name)
                elif rel_type == "ä½äº":
                    location_name = n_name
        if not s_name:
            return ""
        lines = [f"æ™¯åŒºã€{s_name}ã€‘"]
        if s_area:
            lines.append(f"é¢ç§¯ï¼š{s_area}")
        if s_location:
            lines.append(f"ä½ç½®ï¼š{s_location}")
        if location_name:
            lines.append(f"æ‰€åœ¨ï¼š{location_name}")
        if spot_names:
            lines.append("ä¸‹å±æ™¯ç‚¹ï¼š" + "ã€".join(spot_names[:20]))
        if feature_names:
            lines.append("ç‰¹è‰²ï¼š" + "ã€".join(feature_names[:15]))
        if honor_names:
            lines.append("è£èª‰ï¼š" + "ã€".join(honor_names[:10]))
        return "ã€æ™¯åŒºä¸€ç°‡ä¿¡æ¯ã€‘\n" + "\n".join(lines)

    async def _get_scenic_spot_cluster_context(self, scenic_spot_id: int) -> str:
        """æŒ‰ scenic_spot_id æ‹‰å–æ™¯åŒºä¸€ç°‡ï¼ˆå¼‚æ­¥ï¼‰ã€‚"""
        try:
            query = """
            MATCH (s:ScenicSpot {scenic_spot_id: $sid})
            OPTIONAL MATCH (s)-[r]->(n)
            RETURN s, type(r) as rel_type, n
            """
            loop = asyncio.get_event_loop()
            rows = await loop.run_in_executor(
                None,
                neo4j_client.execute_query,
                query,
                {"sid": int(scenic_spot_id)}
            )
            return self._parse_scenic_spot_rows(rows or [])
        except Exception as e:
            logger.warning(f"æ‹‰å–æ™¯åŒºç°‡å¤±è´¥ scenic_spot_id={scenic_spot_id}: {e}")
            return ""

    async def _get_scenic_spot_cluster_context_by_name(self, scenic_name: str) -> str:
        """æŒ‰æ™¯åŒºåç§°æ‹‰å–æ™¯åŒºä¸€ç°‡ï¼ˆå…¼å®¹æ—  scenic_spot_id çš„æ—§èŠ‚ç‚¹ï¼Œå¼‚æ­¥ï¼‰ã€‚"""
        if not (scenic_name or "").strip():
            return ""
        try:
            query = """
            MATCH (s:ScenicSpot {name: $name})
            OPTIONAL MATCH (s)-[r]->(n)
            RETURN s, type(r) as rel_type, n
            """
            loop = asyncio.get_event_loop()
            rows = await loop.run_in_executor(
                None,
                neo4j_client.execute_query,
                query,
                {"name": (scenic_name or "").strip()}
            )
            return self._parse_scenic_spot_rows(rows or [])
        except Exception as e:
            logger.warning(f"æ‹‰å–æ™¯åŒºç°‡å¤±è´¥ï¼ˆæŒ‰åç§°ï¼‰ scenic_name={scenic_name}: {e}")
            return ""

    def _get_text_contents_from_neo4j(self, text_ids: List[str]) -> Dict[str, str]:
        """æŒ‰ text_id ä» Neo4j Text èŠ‚ç‚¹æ‹‰å–æ­£æ–‡ï¼ˆåŒæ­¥æ–¹æ³•ï¼Œç”±è°ƒç”¨æ–¹ç”¨ run_in_executor åŒ…è£…ï¼‰ã€‚"""
        if not text_ids:
            return {}
        result = {}
        try:
            query = """
            MATCH (t:Text) WHERE t.id IN $ids
            RETURN t.id AS id, t.content AS content
            """
            rows = neo4j_client.execute_query(query, {"ids": list(text_ids)})
            for row in rows or []:
                tid = row.get("id")
                content = row.get("content")
                if tid is not None and content:
                    result[str(tid)] = (content if isinstance(content, str) else "").strip()
        except Exception as e:
            logger.warning(f"ä» Neo4j æ‹‰å–æ–‡æœ¬æ­£æ–‡å¤±è´¥: {e}")
        return result

    def _merge_results(self, vector_results: List[Dict], graph_results: List[Dict], entities: List[str]) -> str:
        """èåˆå‘é‡+å›¾æ£€ç´¢ç»“æœä¸ºå¢å¼ºä¸Šä¸‹æ–‡ã€‚"""
        context_parts = []
        if vector_results:
            context_parts.append("ç›¸å…³æ–‡æœ¬å†…å®¹ï¼š")
            for i, result in enumerate(vector_results[:5], 1):
                text_id = result.get("text_id", "")
                score = result.get("score", 0)
                content = result.get("content", "").strip()
                if content:
                    context_parts.append(f"{i}. (ç›¸ä¼¼åº¦: {score:.2f})\n{content}")
                else:
                    context_parts.append(f"{i}. {text_id} (ç›¸ä¼¼åº¦: {score:.2f})")
        if graph_results:
            context_parts.append("\nç›¸å…³å®ä½“å…³ç³»ï¼š")
            seen_relations = set()
            for result in graph_results[:5]:
                if 'a' in result and 'b' in result and 'rel_type' in result:
                    a_name = result['a'].get('name', 'æœªçŸ¥')
                    b_name = result['b'].get('name', 'æœªçŸ¥')
                    rel_type = result.get('rel_type', 'ç›¸å…³')
                    relation_key = f"{a_name}-{rel_type}-{b_name}"
                    if relation_key not in seen_relations:
                        seen_relations.add(relation_key)
                        context_parts.append(f"- {a_name} {rel_type} {b_name}")
        if entities:
            context_parts.append(f"\nè¯†åˆ«åˆ°çš„å®ä½“ï¼š{', '.join(entities[:5])}")
        
        return "\n".join(context_parts)
    
    async def generate_answer(
        self, 
        query: str, 
        context: Optional[str] = None, 
        use_rag: bool = True,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        character_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­”ï¼›RAG ä»…åœ¨å†…éƒ¨æ‰§è¡Œä¸€æ¬¡ã€‚è¿”å› {answer, primary_attraction_id, context}ã€‚"""
        if not self.llm_client:
            return {"answer": "æŠ±æ­‰ï¼ŒAIæœåŠ¡æœªé…ç½®ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚", "primary_attraction_id": None, "context": ""}

        out_context = context or ""
        primary_attraction_id: Optional[int] = None
        rag_debug: Optional[Dict[str, Any]] = None
        needs_context = self._query_needs_context(query) if use_rag else False
        if use_rag and not needs_context:
            out_context = "å½“å‰é—®é¢˜æ— éœ€çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼Œè¯·è‡ªç„¶ã€ç®€çŸ­å›å¤ã€‚"
            rag_debug = {
                "query": query,
                "vector_results": [],
                "graph_results": [],
                "subgraph": None,
                "enhanced_context": out_context,
                "entities": [],
                "skip_rag_reason": "é—®é¢˜ä¸ºå¯’æš„/é€šç”¨é—®ç­”ï¼Œæ— éœ€æ£€ç´¢",
            }
        elif use_rag:
            rag_results = await self.hybrid_search(query, top_k=5)
            primary_attraction_id = rag_results.get("primary_attraction_id")
            out_context = rag_results.get("enhanced_context", "") or ""
            # å¦‚æœ hybrid_search å·²ç»æ‰©å±•äº†æ™¯åŒºä¿¡æ¯ï¼ˆé€šè¿‡ç­–ç•¥ï¼‰ï¼Œè¿™é‡Œä¸å†é‡å¤æŸ¥è¯¢
            # åªåœ¨ hybrid_search æœªæ‰©å±•ä½†ç¡®å®æ˜¯ listing æ„å›¾æ—¶ï¼Œæ‰è¡¥å……
            detected_intent = rag_results.get("intent")
            already_has_list = "æ ¹æ®å›¾æ•°æ®åº“ï¼Œæ™¯åŒºã€Œ" in (out_context or "")
            if detected_intent == "listing" and not already_has_list:
                scenic_ctx = await self._build_scenic_attractions_context(
                    query=query,
                    rag_results=rag_results,
                    conversation_history=conversation_history,
                )
                if scenic_ctx:
                    out_context = f"{out_context}\n\n{scenic_ctx}" if out_context else scenic_ctx
                elif rag_results.get("primary_attraction_id") is not None:
                    try:
                        aid = rag_results.get("primary_attraction_id")
                        parent_info = await self._get_scenic_spot_by_attraction_id(aid)
                        scenic_name = parent_info.get("s_name") if parent_info else None
                        if scenic_name:
                            scenic_ctx = await self._get_scenic_attractions_sentence_by_name(str(scenic_name).strip())
                            if scenic_ctx:
                                out_context = f"{out_context}\n\n{scenic_ctx}" if out_context else scenic_ctx
                    except Exception as e:
                        logger.warning(f"åˆ—ä¸¾æŸ¥è¯¢æ—¶ä»primary_attraction_idåæŸ¥æ™¯åŒºå¤±è´¥: {e}")

            rag_debug = {
                "query": rag_results.get("query") or query,
                "vector_results": rag_results.get("vector_results", [])[:5],
                "graph_results": rag_results.get("graph_results", [])[:5],
                "subgraph": rag_results.get("subgraph"),
                "enhanced_context": out_context or "",
                "entities": rag_results.get("entities", []),
                "errors": rag_results.get("errors", {}),
                "intent": rag_results.get("intent"),  # åŒ…å«æ„å›¾ä¿¡æ¯
                "strategy": rag_results.get("strategy"),  # åŒ…å«ç­–ç•¥ä¿¡æ¯
            }
        base_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™¯åŒºAIå¯¼æ¸¸åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨å‹å¥½ã€ä¸“ä¸šã€å‡†ç¡®çš„è¯­è¨€å›ç­”æ¸¸å®¢çš„é—®é¢˜ã€‚
å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”
2. è¯­è¨€ç®€æ´æ˜äº†ï¼Œé€‚åˆå£è¯­åŒ–è¡¨è¾¾
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯šå®è¯´æ˜
4. ä¸è¦ç¼–é€ ä¿¡æ¯
5. ä¸è¦é€éœ²ä»»ä½•å†…éƒ¨æ ‡è¯†ç¬¦/ç¼–å·/IDï¼ˆä¾‹å¦‚ kb_***ã€text_idã€session_id ç­‰ï¼‰ï¼›è‡ªæˆ‘ä»‹ç»æ—¶ä¹Ÿä¸è¦è¾“å‡ºä»»ä½•â€œç¼–å·â€
6. è¾“å‡ºå†…å®¹å¿…é¡»ä¸ºâ€œå¹²å‡€çš„çº¯æ–‡æœ¬â€ï¼šä¸è¦ä½¿ç”¨ä»»ä½•è¡¨æƒ…/emoji/é¢œæ–‡å­—ï¼Œä¹Ÿä¸è¦ä½¿ç”¨è£…é¥°æ€§ç¬¦å·ï¼ˆä¾‹å¦‚ ï½ã€~ã€ğŸ«¶ã€âœ¨ã€â¤ï¸ ç­‰ï¼‰ã€‚åªä½¿ç”¨æ­£å¸¸ä¸­æ–‡æ ‡ç‚¹ï¼ˆï¼Œã€‚ï¼ï¼Ÿï¼‰ä¸å¿…è¦çš„æ•°å­—/å•ä½ã€‚"""
        if character_prompt:
            system_prompt = f"{base_system_prompt}\n\nè§’è‰²è®¾å®šï¼š{character_prompt}"
        else:
            system_prompt = base_system_prompt
        messages = [{"role": "system", "content": system_prompt}]
        if conversation_history:
            messages.extend(conversation_history)
        # æ ¹æ®æ„å›¾æ·»åŠ é’ˆå¯¹æ€§æç¤ºè¯­
        intent_hint = ""
        if use_rag and rag_debug:
            detected_intent = rag_debug.get("intent") or self._classify_query_intent(query).value
            if detected_intent == "route":
                intent_hint = "è¯´æ˜ï¼šç”¨æˆ·è¯¢é—®çš„æ˜¯æ¸¸ç©/æ¨èè·¯çº¿ï¼Œè¯·ç»“åˆä¸‹åˆ—å¤šä¸ªæ™¯ç‚¹ï¼Œæ¨èä¸€æ¡åˆç†çš„æ¸¸è§ˆé¡ºåºï¼ˆè·¯çº¿ï¼‰ï¼Œå¹¶ç®€è¦è¯´æ˜æ¯æ®µæ€ä¹ˆèµ°æˆ–æ¸¸ç©å»ºè®®ã€‚\n\n"
            elif detected_intent == "listing":
                intent_hint = "è¯´æ˜ï¼šç”¨æˆ·è¯¢é—®çš„æ˜¯æ™¯ç‚¹åˆ—è¡¨æˆ–æ•°é‡ï¼Œè¯·æ¸…æ™°åˆ—å‡ºç›¸å…³æ™¯ç‚¹ï¼Œå¹¶è¯´æ˜æ€»æ•°ã€‚\n\n"
            elif detected_intent == "comparison":
                intent_hint = "è¯´æ˜ï¼šç”¨æˆ·è¯¢é—®çš„æ˜¯æ¯”è¾ƒç±»é—®é¢˜ï¼Œè¯·å¯¹æ¯”ä¸åŒæ™¯ç‚¹çš„ç‰¹ç‚¹ã€ä¼˜åŠ£ï¼Œç»™å‡ºå®¢è§‚å»ºè®®ã€‚\n\n"
            elif detected_intent == "location":
                intent_hint = "è¯´æ˜ï¼šç”¨æˆ·è¯¢é—®çš„æ˜¯ä½ç½®/å¯¼èˆªä¿¡æ¯ï¼Œè¯·é‡ç‚¹è¯´æ˜å…·ä½“ä½ç½®ã€åœ°å€ã€å¦‚ä½•åˆ°è¾¾ã€‚\n\n"
            elif detected_intent == "feature":
                intent_hint = "è¯´æ˜ï¼šç”¨æˆ·è¯¢é—®çš„æ˜¯ç‰¹è‰²/åŠŸèƒ½ï¼Œè¯·é‡ç‚¹è¯´æ˜æ™¯ç‚¹çš„äº®ç‚¹ã€å¥½ç©ä¹‹å¤„ã€æ¨èç†ç”±ã€‚\n\n"
            elif detected_intent == "detail":
                intent_hint = "è¯´æ˜ï¼šç”¨æˆ·è¯¢é—®çš„æ˜¯è¯¦æƒ…/ä»‹ç»ï¼Œè¯·æä¾›å…¨é¢ã€è¯¦ç»†çš„æ™¯ç‚¹ä¿¡æ¯ã€‚\n\n"
        
        user_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{query}
{intent_hint}ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{out_context if out_context else "æ— é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯"}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
        messages.append({"role": "user", "content": user_prompt})
        if rag_debug is not None:
            rag_debug["final_sent_to_llm"] = user_prompt

        try:
            response = self.llm_client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            if answer:
                answer = re.sub(r"ç¼–å·ä¸º\s*kb_\d+", "", answer)
                answer = re.sub(r"\bkb_\d+\b", "", answer)
                answer = re.sub(r"\s{2,}", " ", answer).strip()
            if answer:
                answer = _strip_emoji(answer)
                answer = _clean_special_symbols(answer)
            try:
                log_root = os.path.join(os.path.dirname(os.path.dirname(__file__)), "logs")
                os.makedirs(log_root, exist_ok=True)
                log_path = os.path.join(log_root, "rag_context.log")
                entry = {
                    "timestamp": datetime.utcnow().isoformat() + "Z",
                    "query": query,
                    "character_prompt": character_prompt,
                    "use_rag": use_rag,
                    "rag_debug": rag_debug,
                    "final_answer_preview": answer[:400] if answer else "",
                }
                with open(log_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                try:
                    with open(log_path, "r", encoding="utf-8") as f:
                        lines = [ln for ln in f.readlines() if ln.strip()]
                    if len(lines) > 5:
                        with open(log_path, "w", encoding="utf-8") as f:
                            f.writelines(lines[-5:])
                except Exception:
                    pass
            except Exception as e:
                logger.warning(f"Failed to write RAG context log: {e}")
            logger.info(f"Generated answer for query: {query[:50]}...")
            return {"answer": answer, "primary_attraction_id": primary_attraction_id, "context": out_context}
        except Exception as e:
            logger.error(f"Failed to generate answer: {e}")
            return {"answer": f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}", "primary_attraction_id": None, "context": out_context}


rag_service = RAGService()

