"""GraphRAG æ£€ç´¢æœåŠ¡ï¼šå›¾æ•°æ®åº“ + å‘é‡æ£€ç´¢å¢å¼ºç”Ÿæˆ"""
import logging
import re
import json
import asyncio
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
from app.core.milvus_client import milvus_client
from app.core.neo4j_client import neo4j_client
from app.core.config import settings

logger = logging.getLogger(__name__)

RELEVANCE_SCORE_THRESHOLD = 0.2  # å‘é‡ç›¸ä¼¼åº¦ä¸‹é™ï¼Œä½äºæ­¤å€¼è§†ä¸ºä¸ç›¸å…³

def _strip_emoji(text: str) -> str:
    """å»æ‰è¡¨æƒ…ä¸æœ«å°¾æ§åˆ¶å­—ç¬¦ï¼Œé¿å… TTS å¼‚å¸¸ï¼›ç¼ºå¥å°¾æ—¶è¡¥å¥å·ã€‚"""
    if not text or not isinstance(text, str):
        return text or ""
    # è¯´æ˜ï¼š
    # - æ—§å®ç°åªè¦†ç›–åˆ° U+1F9FFï¼Œåƒ ğŸ«¶(U+1FAF6) è¿™ç§æ–° emoji ä¼šæ¼æ‰
    # - åŒæ—¶æ¸…æ‰ä¸€äº›å¸¸è§â€œè£…é¥°ç¬¦å·â€ï¼ˆå¦‚å…¨è§’æ³¢æµªçº¿ ï½ / æ³¢æµªçº¿ ~ï¼‰ï¼Œé¿å…å½±å“å±•ç¤ºä¸ TTS
    s = re.sub(
        r"[\u2600-\u26FF\u2700-\u27BF"
        r"\U0001F300-\U0001F9FF"
        r"\U0001FA00-\U0001FAFF"  # newer emoji blocks (e.g., ğŸ«¶)
        r"]",
        "",
        text,
    )
    # è£…é¥°æ€§æ³¢æµªçº¿ï¼š~ / ï½(FF5E) / ã€œ(301C)
    s = re.sub(r"[~\uFF5E\u301C]", "", s)
    s = re.sub(r"\s{2,}", " ", s).strip()
    s = re.sub(r"[\s\u200b\u200c\u200d\ufeff\r\n]+$", "", s)
    if s and s[-1] not in "ã€‚ï¼ï¼Ÿ.!?â€¦":
        s = s.rstrip("ï¼Œã€ï¼›ï¼š") + "ã€‚"
    return s


try:
    import jieba
    import jieba.posseg as pseg
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    logger.warning("jieba not available, using simple keyword extraction")

class RAGService:
    """GraphRAGï¼šå®ä½“è¯†åˆ« + Milvus å‘é‡æ£€ç´¢ + Neo4j å›¾æ£€ç´¢ + ç»“æœèåˆã€‚"""

    def __init__(self):
        self.embedding_model = None
        self.llm_client = None
        self._milvus_loaded_collections: set[str] = set()
        self._init_embedding_model()
        self._init_ner()
        self._init_llm_client()

    async def parse_scenic_text(self, text: str) -> Optional[Dict[str, Any]]:
        """å°†æ™¯åŒºä»‹ç»ç»“æ„åŒ–ä¸º JSON ä¾›å›¾åº“å»ºç°‡ï¼›éæ™¯åŒºç±»è¿”å› Noneã€‚"""
        scenic_keywords = ["æ™¯åŒº", "é£æ™¯åŒº", "æ—…æ¸¸åº¦å‡åŒº", "æ™¯ç‚¹", "åº¦å‡åŒº"]
        if not any(k in text for k in scenic_keywords):
            return None

        if not self.llm_client:
            return None

        system_prompt = """
ä½ æ˜¯æ™¯åŒºçŸ¥è¯†ç»“æ„åŒ–åŠ©æ‰‹ã€‚è¯·æŠŠä¸€æ®µä¸­æ–‡æ™¯åŒºä»‹ç»æå–æˆ JSONï¼Œä¸¥æ ¼æŒ‰å­—æ®µè¿”å›ï¼Œä¸è¦å¤šä½™è¯´æ˜ã€‚

è¿”å›å­—æ®µï¼š
- scenic_spot: æ™¯åŒºåç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
- location: è¡Œæ”¿å±‚çº§æ•°ç»„ï¼Œä¾‹å¦‚ ["å››å·çœ", "å®œå®¾å¸‚", "é•¿å®å¿"]ï¼ˆè‹¥ç¼ºå°‘ä¸‹çº§å¯çœç•¥ï¼‰
- area: é¢ç§¯ï¼ˆåŸæ–‡ä¸­çš„æè¿°å­—ç¬¦ä¸²ï¼Œè‹¥æ²¡æœ‰åˆ™ä¸º nullï¼‰
- features: ç‰¹è‰²æ•°ç»„ï¼Œä¾‹å¦‚ ["å¤©ç„¶ç«¹æ—","æ£®æ—è¦†ç›–ç‡93%","æ°”å€™æ¸©å’Œ","é›¨é‡å……æ²›"]
- spots: å­æ™¯ç‚¹æ•°ç»„ï¼Œä¾‹å¦‚ ["ç«¹æµ·åšç‰©é¦†","èŠ±æºªåä¸‰æ¡¥","æµ·ä¸­æµ·","å¤åˆ¹"]
- awards: è£èª‰æ•°ç»„ï¼Œä¾‹å¦‚ ["å›½å®¶é¦–æ‰¹4Açº§æ—…æ¸¸åŒº","å…¨å›½åº·å…»æ—…æ¸¸åŸºåœ°","å›½å®¶çº§æ—…æ¸¸åº¦å‡åŒº"]

åªè¾“å‡º JSON å¯¹è±¡ï¼Œä¸è¦è§£é‡Šã€‚
"""
        try:
            resp = self.llm_client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text},
                ],
                temperature=0.1,
                max_tokens=512,
            )
            raw = resp.choices[0].message.content
            data = json.loads(raw)
            if not isinstance(data, dict):
                return None
            scenic_name = data.get("scenic_spot")
            if not scenic_name or not isinstance(scenic_name, str):
                return None
            data["location"] = data.get("location") or []
            data["features"] = data.get("features") or []
            data["spots"] = data.get("spots") or []
            data["awards"] = data.get("awards") or []
            return data
        except Exception as e:
            logger.warning(f"parse_scenic_text failed: {e}")
            return None

    async def parse_attraction_text(self, name: str, text: str) -> Optional[Dict[str, Any]]:
        """å°†å•æ™¯ç‚¹ä»‹ç»ç»“æ„åŒ–ä¸º JSON ä¾›å›¾åº“å»ºç°‡ã€‚"""
        if not name or not isinstance(name, str):
            return None
        if not self.llm_client:
            return None

        system_prompt = """
ä½ æ˜¯æ™¯ç‚¹çŸ¥è¯†ç»“æ„åŒ–åŠ©æ‰‹ã€‚è¯·æŠŠä¸€æ®µä¸­æ–‡â€œå•ä¸ªæ™¯ç‚¹â€çš„ä»‹ç»æå–æˆ JSONï¼Œä¸¥æ ¼æŒ‰å­—æ®µè¿”å›ï¼Œä¸è¦å¤šä½™è¯´æ˜ã€‚

è¿”å›å­—æ®µï¼š
- name: æ™¯ç‚¹åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
- location: è¡Œæ”¿å±‚çº§æ•°ç»„ï¼Œä¾‹å¦‚ ["å››å·çœ", "å®œå®¾å¸‚", "é•¿å®å¿"]ï¼ˆè‹¥æ— æ³•åˆ¤æ–­åˆ™è¿”å› []ï¼‰
- category: ç±»åˆ«ï¼ˆå­—ç¬¦ä¸²æˆ– nullï¼‰
- features: ç‰¹è‰²/è¦ç‚¹æ•°ç»„ï¼ˆè‹¥æ²¡æœ‰åˆ™ []ï¼‰
- honors: è£èª‰/ç§°å·æ•°ç»„ï¼ˆè‹¥æ²¡æœ‰åˆ™ []ï¼‰

åªè¾“å‡º JSON å¯¹è±¡ï¼Œä¸è¦è§£é‡Šã€‚
"""
        try:
            resp = self.llm_client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"æ™¯ç‚¹åç§°ï¼š{name}\n\n{text}"},
                ],
                temperature=0.1,
                max_tokens=512,
            )
            raw = resp.choices[0].message.content
            data = json.loads(raw)
            if not isinstance(data, dict):
                return None
            if data.get("name") and not isinstance(data.get("name"), str):
                return None
            data["name"] = (data.get("name") or name).strip()
            data["location"] = data.get("location") or []
            data["features"] = data.get("features") or []
            data["honors"] = data.get("honors") or []
            return data
        except Exception as e:
            logger.warning(f"parse_attraction_text failed: {e}")
            return None
    
    def _init_embedding_model(self):
        try:
            self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            logger.info("Embedding model loaded")
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            self.embedding_model = None
    
    def _init_ner(self):
        if JIEBA_AVAILABLE:
            try:
                jieba.initialize()
                logger.info("NER model (jieba) initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize jieba: {e}")
    
    def _init_llm_client(self):
        try:
            if settings.OPENAI_API_KEY:
                import openai
                client_kwargs = {"api_key": settings.OPENAI_API_KEY}
                if settings.OPENAI_API_BASE:
                    client_kwargs["base_url"] = settings.OPENAI_API_BASE
                
                self.llm_client = openai.OpenAI(**client_kwargs)
                logger.info(f"LLM client initialized (base_url: {settings.OPENAI_API_BASE or 'default'})")
            else:
                logger.warning("OpenAI API key not configured, LLM generation disabled")
        except Exception as e:
            logger.error(f"Failed to initialize LLM client: {e}")
            self.llm_client = None
    
    def extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬æå–å®ä½“ï¼Œè¿”å› [{"text", "type", "confidence"}]ã€‚"""
        # è¿‡æ»¤æ‰æ— æ„ä¹‰çš„é€šç”¨è¯
        stop_words = {"è¿™é‡Œ", "é‚£é‡Œ", "å“ªäº›", "ä»€ä¹ˆ", "è¿™ä¸ª", "é‚£ä¸ª", "æ™¯ç‚¹", "æ™¯åŒº", "åœ°æ–¹", 
                      "attraction", "scenic", "spot", "è¿™é‡Œæœ‰å“ªäº›", "æœ‰å“ªäº›æ™¯ç‚¹", "æ™¯ç‚¹éƒ½æœ‰"}
        entities = []
        if JIEBA_AVAILABLE:
            words = pseg.cut(text)
            for word, flag in words:
                # è¿‡æ»¤ï¼šé•¿åº¦>=2ï¼Œä¸æ˜¯åœç”¨è¯ï¼Œä¸”æ˜¯åœ°å/äººå/æœºæ„å/å…¶ä»–ä¸“åï¼Œæˆ–é•¿åº¦>=3çš„è¯æ±‡
                if word in stop_words:
                    continue
                if (flag in ['ns', 'nr', 'nt', 'nz'] or len(word) >= 3) and len(word) >= 2:
                    entities.append({
                        "text": word,
                        "type": self._map_pos_to_entity_type(flag),
                        "confidence": 0.8
                    })
        else:
            pattern = r'[\u4e00-\u9fa5]{2,}'
            matches = re.finditer(pattern, text)
            for match in matches:
                word = match.group()
                if word not in stop_words:
                    entities.append({
                        "text": word,
                        "type": "KEYWORD",
                        "confidence": 0.6
                    })
        seen = set()
        unique_entities = []
        for entity in entities:
            if entity["text"] not in seen:
                seen.add(entity["text"])
                unique_entities.append(entity)
        
        return unique_entities
    
    def _map_pos_to_entity_type(self, pos: str) -> str:
        mapping = {
            'ns': 'LOCATION', 'nr': 'PERSON', 'nt': 'ORG', 'nz': 'OTHER',
        }
        return mapping.get(pos, 'KEYWORD')
    
    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        if not self.embedding_model:
            raise ValueError("Embedding model not loaded")
        
        embedding = self.embedding_model.encode(text, convert_to_numpy=True)
        return embedding.tolist()

    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆæ¯”é€æ¡ encode æ›´å¿«ï¼‰"""
        if not self.embedding_model:
            raise ValueError("Embedding model not loaded")
        if not texts:
            return []
        embs = self.embedding_model.encode(texts, convert_to_numpy=True)
        return embs.tolist()
    
    async def vector_search(self, query: str, collection_name: str = "tour_knowledge", top_k: int = 5) -> List[Dict[str, Any]]:
        """å‘é‡ç›¸ä¼¼åº¦æœç´¢ã€‚"""
        if not milvus_client.connected:
            milvus_client.connect()
        try:
            collection = milvus_client.create_collection_if_not_exists(
                collection_name, dimension=384, load=False
            )
        except Exception as e:
            logger.warning(f"Milvus not available for vector search, fallback to empty results: {e}")
            return []
        try:
            from pymilvus import utility
            if not utility.has_collection(collection_name):
                logger.warning(f"Collection '{collection_name}' does not exist")
                return []
        except Exception as e:
            logger.warning(f"Failed to check collection existence: {e}")
            return []
        if collection_name not in self._milvus_loaded_collections:
            try:
                from pymilvus import utility
                load_state = utility.load_state(collection_name)

                is_loaded = False
                if isinstance(load_state, dict):
                    state_value = (
                        load_state.get("state", "").upper()
                        if isinstance(load_state.get("state"), str)
                        else str(load_state.get("state", "")).upper()
                    )
                    is_loaded = state_value in ("LOADED", "LOADED_FOR_SEARCH")
                elif isinstance(load_state, str):
                    is_loaded = load_state.upper() in ("LOADED", "LOADED_FOR_SEARCH")
                else:
                    is_loaded = "LOADED" in str(load_state).upper()

                if not is_loaded:
                    logger.info(f"Collection '{collection_name}' is not loaded (state: {load_state}), loading now...")
                    collection.load()
                self._milvus_loaded_collections.add(collection_name)
            except Exception as e:
                logger.warning(f"Failed to ensure collection '{collection_name}' loaded, will rely on retry: {e}")
        query_vector = [self.generate_embedding(query)]
        try:
            search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
            results = collection.search(
                data=query_vector,
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                output_fields=["text_id"]
            )
        except Exception as e:
            if "not loaded" in str(e).lower() or "collection not loaded" in str(e).lower():
                logger.warning(f"Search failed due to collection not loaded, retrying after reload: {e}")
                try:
                    collection.load()
                    self._milvus_loaded_collections.add(collection_name)
                    results = collection.search(
                        data=query_vector,
                        anns_field="embedding",
                        param=search_params,
                        limit=top_k,
                        output_fields=["text_id"]
                    )
                except Exception as retry_error:
                    logger.error(f"Retry search failed: {retry_error}")
                    return []
            else:
                logger.error(f"Search failed: {e}")
                return []
        search_results = []
        if results and len(results) > 0:
            for hit in results[0]:
                search_results.append({
                    "id": hit.id,
                    "text_id": hit.entity.get("text_id", ""),
                    "distance": hit.distance,
                    "score": 1 / (1 + hit.distance) if hit.distance > 0 else 1.0,
                })
        
        return search_results
    
    async def graph_search(self, entity_name: str, relation_type: str = None, limit: int = 10) -> List[Dict[str, Any]]:
        """å›¾æ•°æ®åº“å…³ç³»æŸ¥è¯¢ã€‚relation_type ç™½åå•æ ¡éªŒåæ‹¼æ¥ï¼Œé¿å…æ³¨å…¥ã€‚"""
        rel = None
        if relation_type and isinstance(relation_type, str):
            rel_candidate = relation_type.strip().upper()
            if re.match(r"^[A-Z_][A-Z0-9_]*$", rel_candidate):
                rel = rel_candidate

        if rel:
            query = f"""
            MATCH (a)-[r:{rel}]->(b)
            WHERE a.name CONTAINS $name OR b.name CONTAINS $name
            RETURN a, r, b, labels(a) as a_labels, labels(b) as b_labels, type(r) as rel_type
            LIMIT $limit
            """
        else:
            query = """
            MATCH (a)-[r]->(b)
            WHERE a.name CONTAINS $name OR b.name CONTAINS $name
            RETURN a, r, b, labels(a) as a_labels, labels(b) as b_labels, type(r) as rel_type
            LIMIT $limit
            """
        
        results = neo4j_client.execute_query(
            query,
            {"name": entity_name, "limit": limit}
        )
        
        return results
    
    async def graph_subgraph_search(self, entities: List[str], depth: int = 2) -> Dict[str, Any]:
        """åŸºäºå¤šå®ä½“æ„å»ºå­å›¾ã€‚depth ç»æ ¡éªŒåæ‹¼æ¥ï¼ˆNeo4j é™åˆ¶ï¼‰ã€‚"""
        if not entities:
            return {"nodes": [], "relationships": []}
        safe_depth = 2
        try:
            safe_depth = int(depth)
        except Exception:
            safe_depth = 2
        safe_depth = max(1, min(safe_depth, 3))

        query = f"""
        MATCH path = (a)-[*1..{safe_depth}]-(b)
        WHERE a.name IN $entities OR b.name IN $entities
        WITH path, nodes(path) as nodes_list, relationships(path) as rels_list
        UNWIND nodes_list as node
        UNWIND rels_list as rel
        RETURN DISTINCT 
            id(node) as node_id,
            labels(node) as labels,
            properties(node) as properties,
            id(rel) as rel_id,
            type(rel) as rel_type,
            properties(rel) as rel_properties
        LIMIT 50
        """
        results = neo4j_client.execute_query(query, {"entities": entities})
        nodes = {}
        relationships = []
        
        for record in results:
            if 'node_id' in record:
                node_id = record['node_id']
                if node_id not in nodes:
                    nodes[node_id] = {
                        "id": node_id,
                        "labels": record.get('labels', []),
                        "properties": record.get('properties', {})
                    }
            
            if 'rel_id' in record and record['rel_id']:
                relationships.append({
                    "id": record['rel_id'],
                    "type": record.get('rel_type'),
                    "properties": record.get('rel_properties', {})
                })
        
        return {
            "nodes": list(nodes.values()),
            "relationships": relationships,
            "entity_count": len(entities)
        }
    
    def _query_needs_context(self, query: str) -> bool:
        """å¯’æš„/è‡´è°¢/å‘Šåˆ«/èƒ½åŠ›è¯¢é—®ç­‰è¿”å› Falseï¼Œä¸æ£€ç´¢ï¼›æ™¯åŒº/æ™¯ç‚¹é—®é¢˜æ‰èµ° RAGã€‚"""
        if not query or not isinstance(query, str):
            return False
        q = query.strip()
        if len(q) <= 1:
            return False
        no_context_patterns = [
            r"^(ä½ å¥½|æ‚¨å¥½|å—¨|hello|hi|åœ¨å—|åœ¨ä¸åœ¨)\s*[ï¼Ÿ?]?$",
            r"^(è°¢è°¢|æ„Ÿè°¢|å¤šè°¢|è°¢è°¢æ‚¨)\s*[ï¼!ã€‚.]?$",
            r"^(å†è§|æ‹œæ‹œ|bye)\s*[ï¼!ã€‚.]?$",
            r"^(ä½ æ˜¯è°|ä½ èƒ½åšä»€ä¹ˆ|æœ‰ä»€ä¹ˆåŠŸèƒ½|ä½ èƒ½å¹²å˜›|ä»‹ç»ä¸‹è‡ªå·±)\s*[ï¼Ÿ?]?$",
            r"^(å¸®åŠ©|help|æ€ä¹ˆç”¨|å¦‚ä½•ä½¿ç”¨)\s*[ï¼Ÿ?]?$",
            r"^éšä¾¿(é—®é—®|é—®é—®çœ‹)?\s*[ï¼Ÿ?]?$",
        ]
        for pat in no_context_patterns:
            if re.search(pat, q, re.IGNORECASE):
                return False
        return True
    
    def _is_listing_query(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºâ€œæ™¯ç‚¹åˆ—è¡¨/æ•°é‡â€ç±»é—®é¢˜ï¼Œä¾‹å¦‚æœ‰å“ªäº›æ™¯ç‚¹ã€æ™¯ç‚¹åˆ†å¸ƒã€å¤šå°‘ä¸ªæ™¯ç‚¹ç­‰ã€‚"""
        if not query or not isinstance(query, str):
            return False
        q = query.strip()
        if not q:
            return False
        # åŒæ—¶è¦†ç›–â€œæœ‰å“ªäº›æ™¯ç‚¹â€å’Œâ€œæœ‰å¤šå°‘ä¸ªæ™¯ç‚¹/å‡ ä¸ªæ™¯ç‚¹â€è¿™ä¸¤ç±»æ„å›¾
        pattern = (
            r"æœ‰å“ªäº›æ™¯ç‚¹|æ™¯ç‚¹éƒ½æœ‰(ä»€ä¹ˆ|å“ªäº›)|æ™¯ç‚¹æƒ…å†µ|æ™¯ç‚¹åˆ†å¸ƒ|æœ‰ä»€ä¹ˆæ™¯ç‚¹|æ™¯ç‚¹.*æœ‰å“ªäº›"
            r"|æœ‰å¤šå°‘ä¸ªæ™¯ç‚¹|å¤šå°‘ä¸ªæ™¯ç‚¹|æ™¯ç‚¹æœ‰å¤šå°‘ä¸ª|æ™¯åŒºæœ‰å¤šå°‘ä¸ªæ™¯ç‚¹|å‡ ä¸ªæ™¯ç‚¹"
        )
        return bool(re.search(pattern, q))

    def _get_scenic_spot_by_attraction_id(self, attraction_id: int) -> Optional[Dict[str, Any]]:
        """é€šè¿‡æ™¯ç‚¹ id åæŸ¥æ‰€å±æ™¯åŒºï¼Œè¿”å› {'sid', 's_name'} æˆ– Noneã€‚"""
        try:
            query = """
            MATCH (a:Attraction {id: $aid})
            OPTIONAL MATCH (a)-[:å±äº]->(s1:ScenicSpot)
            OPTIONAL MATCH (s2:ScenicSpot)-[:HAS_SPOT]->(a)
            WITH a, coalesce(s1, s2) AS s WHERE s IS NOT NULL
            RETURN s.scenic_spot_id AS sid, s.name AS s_name
            LIMIT 1
            """
            rows = neo4j_client.execute_query(query, {"aid": int(attraction_id)})
            if rows:
                row0 = rows[0]
                return {
                    "sid": row0.get("sid"),
                    "s_name": row0.get("s_name"),
                }
        except Exception as e:
            logger.warning(f"_get_scenic_spot_by_attraction_id failed attraction_id={attraction_id}: {e}")
        return None

    def _get_scenic_attractions_sentence_by_name(self, scenic_name: str) -> str:
        """æ ¹æ®æ™¯åŒºåç§°æŸ¥è¯¢å…¶ä¸‹ç›¸å…³æ™¯ç‚¹ï¼Œå¹¶æ ¼å¼åŒ–ä¸ºä¸€å¥è¯æè¿°ï¼ˆå¸¦æ•°é‡ä¿¡æ¯ï¼‰ã€‚"""
        scenic_name = (scenic_name or "").strip()
        if not scenic_name:
            return ""
        try:
            scenic_attractions_q = """
            MATCH (s:ScenicSpot {name: $name})
            OPTIONAL MATCH (s)-[:HAS_SPOT]->(n)
            OPTIONAL MATCH (s)<-[:å±äº]-(a:Attraction)
            WITH s, collect(DISTINCT n) AS spot_list, collect(DISTINCT a) AS att_list
            UNWIND spot_list + att_list AS x
            WITH s, x WHERE x IS NOT NULL
            WITH DISTINCT s, x, coalesce(x.name, x.text_id) AS xname
            WHERE xname IS NOT NULL AND NOT (xname STARTS WITH 'kb_')
            RETURN s.name AS scenic_name, xname AS attraction_name
            ORDER BY attraction_name
            LIMIT 50
            """
            rows = neo4j_client.execute_query(scenic_attractions_q, {"name": scenic_name}) or []
        except Exception as e:
            logger.warning(f"_get_scenic_attractions_sentence_by_name query failed scenic_name={scenic_name}: {e}")
            return ""

        attraction_names: List[str] = []
        for row in rows:
            nm = row.get("attraction_name")
            if not nm or nm in attraction_names:
                continue
            attraction_names.append(nm)
        if not attraction_names:
            return ""
        count = len(attraction_names)
        joined = "ã€".join(attraction_names)
        return f"æ ¹æ®å›¾æ•°æ®åº“ï¼Œæ™¯åŒºã€Œ{scenic_name}ã€ä¸‹çš„ç›¸å…³æ™¯ç‚¹å…±æœ‰ {count} ä¸ªï¼ŒåŒ…æ‹¬ï¼š{joined}ã€‚"

    async def hybrid_search(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """å‘é‡æ£€ç´¢ + å®ä½“è¯†åˆ« + å›¾æ£€ç´¢ + ç»“æœèåˆï¼ˆå¹¶è¡Œä¼˜åŒ–ï¼‰ã€‚"""
        vector_results = await self.vector_search(query, top_k=top_k)
        vector_results_relevant = [r for r in (vector_results or []) if (r.get("score") or 0) >= RELEVANCE_SCORE_THRESHOLD]
        if not vector_results_relevant and vector_results:
            vector_results_relevant = vector_results[:1]
        vector_results = vector_results_relevant

        # æ‰¹é‡æå–å®ä½“ï¼Œå‡å°‘é‡å¤å¤„ç†
        texts_to_extract = [query]
        if vector_results:
            text_ids = [r.get("text_id", "") for r in vector_results[:3] if r.get("text_id")]
            texts_to_extract.extend(text_ids)
        
        # å¹¶è¡Œæå–å®ä½“ï¼ˆå¦‚æœæ–‡æœ¬è¾ƒå¤šï¼‰
        if len(texts_to_extract) > 1:
            loop = asyncio.get_event_loop()
            entities_list = await asyncio.gather(*[
                loop.run_in_executor(None, self.extract_entities, text)
                for text in texts_to_extract
            ])
            entities = []
            for ent_list in entities_list:
                entities.extend(ent_list)
        else:
            entities = self.extract_entities(query)
        
        # ä½¿ç”¨å­—å…¸å»é‡ï¼Œä¿ç•™æœ€é«˜ç½®ä¿¡åº¦
        unique_entities = {}
        for entity in entities:
            text = entity["text"]
            if text not in unique_entities or entity["confidence"] > unique_entities[text]["confidence"]:
                unique_entities[text] = entity
        
        entity_names = [e["text"] for e in unique_entities.values()]
        
        text_ids_to_fetch = [
            (r.get("text_id") or "").strip()
            for r in (vector_results or [])
            if (r.get("text_id") or "").strip() and not (r.get("text_id") or "").strip().startswith("attraction_")
        ]
        
        tasks = []
        if entity_names:
            tasks.extend([self.graph_search(name, limit=5) for name in entity_names[:5]])
            if len(entity_names) > 1:
                tasks.append(self.graph_subgraph_search(entity_names[:3], depth=2))
        
        graph_results = []
        subgraph_data = None
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            task_idx = 0
            for name in entity_names[:5]:
                if task_idx < len(results):
                    r = results[task_idx]
                    if not isinstance(r, Exception):
                        graph_results.extend(r)
                    task_idx += 1
            if len(entity_names) > 1 and task_idx < len(results):
                r = results[task_idx]
                if not isinstance(r, Exception):
                    subgraph_data = r
        
        # å¹¶è¡Œæ‰§è¡Œæ–‡æœ¬å†…å®¹è·å–ï¼ˆä½¿ç”¨çº¿ç¨‹æ± é¿å…é˜»å¡ï¼‰
        text_contents = {}
        if text_ids_to_fetch:
            loop = asyncio.get_event_loop()
            text_contents = await loop.run_in_executor(
                None, 
                self._get_text_contents_from_neo4j, 
                text_ids_to_fetch
            )
        
        for r in vector_results or []:
            tid = (r.get("text_id") or "").strip()
            if tid and tid in text_contents:
                r["content"] = text_contents[tid]
        enhanced_results = self._merge_results(vector_results, graph_results, entity_names)
        attraction_ids = []
        primary_attraction_id = None
        for r in (vector_results or []):
            text_id = (r.get("text_id") or "").strip()
            if text_id.startswith("attraction_"):
                try:
                    aid = int(text_id.replace("attraction_", ""))
                    attraction_ids.append(aid)
                    if primary_attraction_id is None:
                        primary_attraction_id = aid
                except ValueError:
                    pass
        
        # æ£€æµ‹åˆ—ä¸¾ç±»æŸ¥è¯¢ï¼ˆ"æœ‰å“ªäº›æ™¯ç‚¹"ï¼‰ï¼Œä¸»åŠ¨æŸ¥è¯¢æ™¯åŒºä¸‹çš„æ‰€æœ‰æ™¯ç‚¹
        is_listing_query = self._is_listing_query(query)
        if is_listing_query and primary_attraction_id is not None:
            # é€šè¿‡æ‰¾åˆ°çš„æ™¯ç‚¹åæŸ¥æ‰€å±æ™¯åŒº
            try:
                parent_info = self._get_scenic_spot_by_attraction_id(primary_attraction_id)
                if parent_info:
                    s_name = parent_info.get("s_name")
                    if s_name:
                        # å…ˆæ‹‰å–è¯¥æ™¯åŒºä¸‹æ‰€æœ‰æ™¯ç‚¹çš„ idï¼ˆç”¨äºæ‹¼â€œæ‰€æœ‰æ™¯ç‚¹ç°‡â€ï¼‰
                        scenic_aids: List[int] = []
                        try:
                            scenic_aids_q = """
                            MATCH (s:ScenicSpot {name: $name})
                            OPTIONAL MATCH (s)<-[:å±äº]-(a:Attraction)
                            OPTIONAL MATCH (s)-[:HAS_SPOT]->(a2:Attraction)
                            WITH collect(DISTINCT a) + collect(DISTINCT a2) AS xs
                            UNWIND xs AS x
                            WITH DISTINCT x WHERE x IS NOT NULL AND x.id IS NOT NULL
                            RETURN x.id AS aid
                            ORDER BY aid
                            LIMIT 200
                            """
                            rows = neo4j_client.execute_query(scenic_aids_q, {"name": str(s_name).strip()}) or []
                            for rr in rows:
                                if rr and rr.get("aid") is not None:
                                    try:
                                        scenic_aids.append(int(rr["aid"]))
                                    except Exception:
                                        continue
                        except Exception as e:
                            logger.warning(f"æŸ¥è¯¢æ™¯åŒºä¸‹æ™¯ç‚¹idå¤±è´¥: {e}")
                        sentence = self._get_scenic_attractions_sentence_by_name(str(s_name).strip())
                        if sentence:
                            enhanced_results = sentence + "\n\n" + (enhanced_results or "")

                        # åˆ—ä¸¾ç±»é—®é¢˜ï¼šå°½é‡æŠŠâ€œè¯¥æ™¯åŒºä¸‹çš„æ‰€æœ‰æ™¯ç‚¹ç°‡ä¿¡æ¯â€ä¹Ÿæ‹¼è¿›å»
                        if scenic_aids:
                            clusters_ctx = await self._get_attraction_cluster_context(scenic_aids, max_items=30)
                            if clusters_ctx:
                                enhanced_results = (enhanced_results or "") + "\n\n" + clusters_ctx
            except Exception as e:
                logger.warning(f"åˆ—ä¸¾æŸ¥è¯¢æ—¶æŸ¥è¯¢æ™¯åŒºæ™¯ç‚¹å¤±è´¥: {e}")
        query_about_scenic = bool(re.search(r"ä»€ä¹ˆæ™¯åŒº|å“ªä¸ªæ™¯åŒº|æ˜¯å•¥æ™¯åŒº|è¿™æ˜¯ä»€ä¹ˆæ™¯åŒº|æ˜¯å“ªä¸ªæ™¯åŒº|å•¥æ™¯åŒº|å“ªä¸ªæ™¯ç‚¹.*æ™¯åŒº|ä»‹ç».*æ™¯åŒº|æ™¯åŒº.*ä»‹ç»|è¿™ä¸ªæ™¯åŒº", (query or "").strip()))
        scenic_ctx_found = False
        if query_about_scenic:
            scenic_tasks = []
            if primary_attraction_id is not None:
                async def get_scenic_from_attraction():
                    try:
                        parent_info = self._get_scenic_spot_by_attraction_id(primary_attraction_id)
                        if parent_info:
                            sid = parent_info.get("sid")
                            s_name = parent_info.get("s_name")
                            if sid is not None:
                                return await self._get_scenic_spot_cluster_context(int(sid))
                            if s_name:
                                return await self._get_scenic_spot_cluster_context_by_name(str(s_name).strip())
                    except Exception as e:
                        logger.warning(f"æŸ¥è¯¢æ™¯ç‚¹æ‰€å±æ™¯åŒºå¤±è´¥: {e}")
                    return ""
                scenic_tasks.append(get_scenic_from_attraction())
            
            if entity_names:
                async def get_scenic_from_entity(name):
                    try:
                        scenic_check_q = """
                        MATCH (s:ScenicSpot {name: $name})
                        RETURN s.scenic_spot_id AS sid, s.name AS s_name
                        LIMIT 1
                        """
                        scenic_rows = neo4j_client.execute_query(scenic_check_q, {"name": name})
                        if scenic_rows:
                            row0 = scenic_rows[0]
                            sid = row0.get("sid")
                            s_name = row0.get("s_name")
                            if sid is not None:
                                return await self._get_scenic_spot_cluster_context(int(sid))
                            if s_name:
                                return await self._get_scenic_spot_cluster_context_by_name(str(s_name).strip())
                    except Exception as e:
                        logger.warning(f"ä»å®ä½“åç§°æŸ¥æ‰¾æ™¯åŒºå¤±è´¥: {e}")
                    return ""
                for entity_name in entity_names[:3]:
                    scenic_tasks.append(get_scenic_from_entity(entity_name))
            
            if subgraph_data:
                async def get_scenic_from_subgraph():
                    for node in subgraph_data.get("nodes", []):
                        labels = set(node.get("labels") or [])
                        props = node.get("properties") or {}
                        if "ScenicSpot" in labels and isinstance(props.get("name"), str):
                            scenic_name = props["name"]
                            try:
                                return await self._get_scenic_spot_cluster_context_by_name(scenic_name)
                            except Exception as e:
                                logger.warning(f"ä»å­å›¾æŸ¥æ‰¾æ™¯åŒºå¤±è´¥: {e}")
                                continue
                    return ""
                scenic_tasks.append(get_scenic_from_subgraph())
            
            if scenic_tasks:
                scenic_results = await asyncio.gather(*scenic_tasks, return_exceptions=True)
                for scenic_ctx in scenic_results:
                    if scenic_ctx and not isinstance(scenic_ctx, Exception) and scenic_ctx.strip():
                        enhanced_results = scenic_ctx + "\n\n" + (enhanced_results or "")
                        scenic_ctx_found = True
                        break
        # åˆ—ä¸¾ç±»æŸ¥è¯¢é€šå¸¸å·²æ‹¼æ¥â€œå¤šä¸ªæ™¯ç‚¹ç°‡â€ï¼Œè¿™é‡Œä¸å†è¡¥ primary å•ç°‡ï¼Œé¿å…é‡å¤
        is_listing_query = self._is_listing_query(query)
        if primary_attraction_id is not None and (not is_listing_query) and not (query_about_scenic and scenic_ctx_found):
            cluster_ctx = await self._get_attraction_cluster_context([primary_attraction_id], max_items=1)
            if cluster_ctx:
                enhanced_results = (enhanced_results or "") + "\n\n" + cluster_ctx
        
        return {
            "vector_results": vector_results,
            "graph_results": graph_results,
            "subgraph": subgraph_data,
            "entities": list(unique_entities.values()),
            "enhanced_context": enhanced_results,
            "query": query,
            "attraction_ids": attraction_ids,
            "primary_attraction_id": primary_attraction_id,
        }

    async def _build_scenic_attractions_context(
        self,
        query: str,
        rag_results: Dict[str, Any],
        conversation_history: Optional[List[Dict[str, str]]] = None,
    ) -> str:
        """åˆ—ä¸¾æ™¯ç‚¹ç±»é—®é¢˜æ—¶ï¼Œè¡¥å……â€œæŸæ™¯åŒºä¸‹æœ‰å“ªäº›æ™¯ç‚¹â€çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚"""
        if not self._is_listing_query(query):
            return ""

        scenic_names: set[str] = set()
        subgraph = rag_results.get("subgraph") or {}
        for node in subgraph.get("nodes", []):
            labels = set(node.get("labels") or [])
            props = node.get("properties") or {}
            if "ScenicSpot" in labels and isinstance(props.get("name"), str):
                scenic_names.add(props["name"])
        if not scenic_names and conversation_history:
            recent_user_text = "ã€‚".join(
                msg["content"]
                for msg in conversation_history[-6:]
                if msg.get("role") == "user" and isinstance(msg.get("content"), str)
            )
            if recent_user_text:
                for ent in self.extract_entities(recent_user_text):
                    cand = ent.get("text")
                    if not cand or not isinstance(cand, str):
                        continue
                    try:
                        check_q = """
                        MATCH (s:ScenicSpot {name: $name})
                        RETURN s.name AS name
                        LIMIT 1
                        """
                        res = neo4j_client.execute_query(check_q, {"name": cand})
                        if res and isinstance(res, list) and res[0].get("name"):
                            scenic_names.add(res[0]["name"])
                    except Exception:
                        continue
        if not scenic_names:
            try:
                all_scenic_q = """
                MATCH (s:ScenicSpot) RETURN s.name AS name LIMIT 5
                """
                rows = neo4j_client.execute_query(all_scenic_q, {}) or []
                for row in rows:
                    nm = row.get("name")
                    if nm and isinstance(nm, str):
                        scenic_names.add(nm)
            except Exception as e:
                logger.warning(f"query all ScenicSpot names failed: {e}")

        if not scenic_names:
            return ""
        parts: List[str] = []
        for scenic_name in list(scenic_names)[:3]:
            sentence = self._get_scenic_attractions_sentence_by_name(scenic_name)
            if sentence:
                parts.append(sentence)

        return "\n".join(parts)
    
    def _get_node_name(self, node: Any) -> str:
        """Neo4j èŠ‚ç‚¹å®‰å…¨å– nameã€‚"""
        if node is None:
            return ""
        if isinstance(node, dict):
            return (node.get("name") or (node.get("properties") or {}).get("name") or "").strip()
        if hasattr(node, "get"):
            return (node.get("name") or "").strip()
        return ""

    async def _get_attraction_cluster_context(self, attraction_ids: List[int], max_items: int = 20) -> str:
        """ä» Neo4j æ‹‰å–æ™¯ç‚¹ä¸€ç°‡ï¼ˆå±æ€§+å‡ºè¾¹ï¼‰ï¼Œæ ¼å¼åŒ–ä¸ºæ–‡æœ¬ä¾› LLMï¼ˆå¹¶è¡Œä¼˜åŒ–ï¼‰ã€‚

        max_items: ä¸ºé¿å…ä¸€æ¬¡æ€§å¡å…¥è¿‡å¤šç°‡å¯¼è‡´ä¸Šä¸‹æ–‡æˆªæ–­ï¼Œåšå®‰å…¨ä¸Šé™ï¼ˆåˆ—ä¸¾ç±»é—®é¢˜å»ºè®® 20 å·¦å³ï¼‰ã€‚
        """
        if not attraction_ids:
            return ""
        
        # å¹¶è¡ŒæŸ¥è¯¢å¤šä¸ªæ™¯ç‚¹
        async def fetch_attraction_cluster(aid: int):
            try:
                query = """
                MATCH (a:Attraction {id: $id})
                OPTIONAL MATCH (a)-[r]->(n)
                RETURN a, type(r) as rel_type, n
                """
                loop = asyncio.get_event_loop()
                rows = await loop.run_in_executor(
                    None,
                    neo4j_client.execute_query,
                    query,
                    {"id": int(aid)}
                )
                if not rows:
                    return None
                att_name = ""
                att_desc = ""
                att_location = ""
                att_category = ""
                relations = []
                for row in rows:
                    a = row.get("a")
                    rel_type = row.get("rel_type")
                    n = row.get("n")
                    if a is not None and not att_name:
                        att_name = self._get_node_name(a) or (str(a.get("id")) if hasattr(a, "get") else "")
                        if hasattr(a, "get"):
                            att_desc = (a.get("description") or "").strip()
                            att_location = (a.get("location") or "").strip()
                            att_category = (a.get("category") or "").strip()
                        elif isinstance(a, dict):
                            att_desc = (a.get("description") or (a.get("properties") or {}).get("description") or "").strip()
                            att_location = (a.get("location") or (a.get("properties") or {}).get("location") or "").strip()
                            att_category = (a.get("category") or (a.get("properties") or {}).get("category") or "").strip()
                    if rel_type and n is not None:
                        n_name = self._get_node_name(n)
                        if n_name:
                            relations.append(f"{rel_type} -> {n_name}")
                if not att_name and not relations:
                    return None
                cluster_lines = [f"æ™¯ç‚¹ã€{att_name or ('ID:' + str(aid))}ã€‘"]
                if att_desc:
                    cluster_lines.append(f"æè¿°ï¼š{att_desc}")
                if att_location:
                    cluster_lines.append(f"ä½ç½®ï¼š{att_location}")
                if att_category:
                    cluster_lines.append(f"ç±»åˆ«ï¼š{att_category}")
                if relations:
                    cluster_lines.append("å…³ç³»ä¸å±æ€§ï¼š" + "ï¼›".join(relations))
                return "\n".join(cluster_lines)
            except Exception as e:
                logger.warning(f"æ‹‰å–æ™¯ç‚¹ç°‡å¤±è´¥ attraction_id={aid}: {e}")
                return None
        
        # å¹¶è¡Œè·å–æ‰€æœ‰æ™¯ç‚¹ç°‡
        # å»é‡ + æˆªæ–­ï¼ˆä¿åºï¼‰
        unique_ids: List[int] = []
        seen_ids: set[int] = set()
        for aid in attraction_ids:
            try:
                ia = int(aid)
            except Exception:
                continue
            if ia in seen_ids:
                continue
            seen_ids.add(ia)
            unique_ids.append(ia)
            if len(unique_ids) >= max(1, min(int(max_items), 80)):
                break
        results = await asyncio.gather(*[fetch_attraction_cluster(aid) for aid in unique_ids], return_exceptions=True)
        parts = [r for r in results if r and not isinstance(r, Exception)]
        
        if not parts:
            return ""
        return "ã€æ™¯ç‚¹ä¸€ç°‡ä¿¡æ¯ã€‘\n" + "\n\n".join(parts)

    def _parse_scenic_spot_rows(self, rows: List[Dict]) -> str:
        """è§£æ ScenicSpot è¡Œä¸ºæ™¯åŒºä¸€ç°‡æ–‡æœ¬ï¼Œä¾›æŒ‰ id/name å…±ç”¨ã€‚"""
        if not rows:
            return ""
        s_name = ""
        s_area = ""
        s_location = ""
        spot_names = []
        feature_names = []
        honor_names = []
        location_name = ""
        for row in rows or []:
            s = row.get("s")
            rel_type = row.get("rel_type")
            n = row.get("n")
            if s is not None and not s_name:
                if hasattr(s, "get"):
                    s_name = (s.get("name") or "").strip()
                    s_area = (s.get("area") or "").strip()
                    s_location = (s.get("location") or "").strip()
                elif isinstance(s, dict):
                    s_name = (s.get("name") or (s.get("properties") or {}).get("name") or "").strip()
                    s_area = (s.get("area") or (s.get("properties") or {}).get("area") or "").strip()
                    s_location = (s.get("location") or (s.get("properties") or {}).get("location") or "").strip()
            if rel_type and n is not None:
                n_name = self._get_node_name(n)
                if not n_name:
                    continue
                if rel_type == "HAS_SPOT":
                    spot_names.append(n_name)
                elif rel_type == "HAS_FEATURE":
                    feature_names.append(n_name)
                elif rel_type == "HAS_HONOR":
                    honor_names.append(n_name)
                elif rel_type == "ä½äº":
                    location_name = n_name
        if not s_name:
            return ""
        lines = [f"æ™¯åŒºã€{s_name}ã€‘"]
        if s_area:
            lines.append(f"é¢ç§¯ï¼š{s_area}")
        if s_location:
            lines.append(f"ä½ç½®ï¼š{s_location}")
        if location_name:
            lines.append(f"æ‰€åœ¨ï¼š{location_name}")
        if spot_names:
            lines.append("ä¸‹å±æ™¯ç‚¹ï¼š" + "ã€".join(spot_names[:20]))
        if feature_names:
            lines.append("ç‰¹è‰²ï¼š" + "ã€".join(feature_names[:15]))
        if honor_names:
            lines.append("è£èª‰ï¼š" + "ã€".join(honor_names[:10]))
        return "ã€æ™¯åŒºä¸€ç°‡ä¿¡æ¯ã€‘\n" + "\n".join(lines)

    async def _get_scenic_spot_cluster_context(self, scenic_spot_id: int) -> str:
        """æŒ‰ scenic_spot_id æ‹‰å–æ™¯åŒºä¸€ç°‡ã€‚"""
        try:
            query = """
            MATCH (s:ScenicSpot {scenic_spot_id: $sid})
            OPTIONAL MATCH (s)-[r]->(n)
            RETURN s, type(r) as rel_type, n
            """
            rows = neo4j_client.execute_query(query, {"sid": int(scenic_spot_id)})
            return self._parse_scenic_spot_rows(rows or [])
        except Exception as e:
            logger.warning(f"æ‹‰å–æ™¯åŒºç°‡å¤±è´¥ scenic_spot_id={scenic_spot_id}: {e}")
            return ""

    async def _get_scenic_spot_cluster_context_by_name(self, scenic_name: str) -> str:
        """æŒ‰æ™¯åŒºåç§°æ‹‰å–æ™¯åŒºä¸€ç°‡ï¼ˆå…¼å®¹æ—  scenic_spot_id çš„æ—§èŠ‚ç‚¹ï¼‰ã€‚"""
        if not (scenic_name or "").strip():
            return ""
        try:
            query = """
            MATCH (s:ScenicSpot {name: $name})
            OPTIONAL MATCH (s)-[r]->(n)
            RETURN s, type(r) as rel_type, n
            """
            rows = neo4j_client.execute_query(query, {"name": (scenic_name or "").strip()})
            return self._parse_scenic_spot_rows(rows or [])
        except Exception as e:
            logger.warning(f"æ‹‰å–æ™¯åŒºç°‡å¤±è´¥ï¼ˆæŒ‰åç§°ï¼‰ scenic_name={scenic_name}: {e}")
            return ""

    def _get_text_contents_from_neo4j(self, text_ids: List[str]) -> Dict[str, str]:
        """æŒ‰ text_id ä» Neo4j Text èŠ‚ç‚¹æ‹‰å–æ­£æ–‡ã€‚"""
        if not text_ids:
            return {}
        result = {}
        try:
            query = """
            MATCH (t:Text) WHERE t.id IN $ids
            RETURN t.id AS id, t.content AS content
            """
            rows = neo4j_client.execute_query(query, {"ids": list(text_ids)})
            for row in rows or []:
                tid = row.get("id")
                content = row.get("content")
                if tid is not None and content:
                    result[str(tid)] = (content if isinstance(content, str) else "").strip()
        except Exception as e:
            logger.warning(f"ä» Neo4j æ‹‰å–æ–‡æœ¬æ­£æ–‡å¤±è´¥: {e}")
        return result

    def _merge_results(self, vector_results: List[Dict], graph_results: List[Dict], entities: List[str]) -> str:
        """èåˆå‘é‡+å›¾æ£€ç´¢ç»“æœä¸ºå¢å¼ºä¸Šä¸‹æ–‡ã€‚"""
        context_parts = []
        if vector_results:
            context_parts.append("ç›¸å…³æ–‡æœ¬å†…å®¹ï¼š")
            for i, result in enumerate(vector_results[:5], 1):
                text_id = result.get("text_id", "")
                score = result.get("score", 0)
                content = result.get("content", "").strip()
                if content:
                    context_parts.append(f"{i}. (ç›¸ä¼¼åº¦: {score:.2f})\n{content}")
                else:
                    context_parts.append(f"{i}. {text_id} (ç›¸ä¼¼åº¦: {score:.2f})")
        if graph_results:
            context_parts.append("\nç›¸å…³å®ä½“å…³ç³»ï¼š")
            seen_relations = set()
            for result in graph_results[:5]:
                if 'a' in result and 'b' in result and 'rel_type' in result:
                    a_name = result['a'].get('name', 'æœªçŸ¥')
                    b_name = result['b'].get('name', 'æœªçŸ¥')
                    rel_type = result.get('rel_type', 'ç›¸å…³')
                    relation_key = f"{a_name}-{rel_type}-{b_name}"
                    if relation_key not in seen_relations:
                        seen_relations.add(relation_key)
                        context_parts.append(f"- {a_name} {rel_type} {b_name}")
        if entities:
            context_parts.append(f"\nè¯†åˆ«åˆ°çš„å®ä½“ï¼š{', '.join(entities[:5])}")
        
        return "\n".join(context_parts)
    
    async def generate_answer(
        self, 
        query: str, 
        context: Optional[str] = None, 
        use_rag: bool = True,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        character_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­”ï¼›RAG ä»…åœ¨å†…éƒ¨æ‰§è¡Œä¸€æ¬¡ã€‚è¿”å› {answer, primary_attraction_id, context}ã€‚"""
        if not self.llm_client:
            return {"answer": "æŠ±æ­‰ï¼ŒAIæœåŠ¡æœªé…ç½®ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚", "primary_attraction_id": None, "context": ""}

        out_context = context or ""
        primary_attraction_id: Optional[int] = None
        rag_debug: Optional[Dict[str, Any]] = None
        needs_context = self._query_needs_context(query) if use_rag else False
        if use_rag and not needs_context:
            out_context = "å½“å‰é—®é¢˜æ— éœ€çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼Œè¯·è‡ªç„¶ã€ç®€çŸ­å›å¤ã€‚"
            rag_debug = {
                "query": query,
                "vector_results": [],
                "graph_results": [],
                "subgraph": None,
                "enhanced_context": out_context,
                "entities": [],
                "skip_rag_reason": "é—®é¢˜ä¸ºå¯’æš„/é€šç”¨é—®ç­”ï¼Œæ— éœ€æ£€ç´¢",
            }
        elif use_rag:
            rag_results = await self.hybrid_search(query, top_k=5)
            primary_attraction_id = rag_results.get("primary_attraction_id")
            out_context = rag_results.get("enhanced_context", "") or ""
            # æ£€æµ‹åˆ—ä¸¾ç±»æŸ¥è¯¢ï¼Œä¸»åŠ¨è¡¥å……æ™¯åŒºä¸‹çš„æ‰€æœ‰æ™¯ç‚¹ä¿¡æ¯
            if self._is_listing_query(query):
                scenic_ctx = await self._build_scenic_attractions_context(
                    query=query,
                    rag_results=rag_results,
                    conversation_history=conversation_history,
                )
                # é¿å…é‡å¤ï¼ˆhybrid_search å¯èƒ½å·²ç»æ‹¼è¿‡â€œæ ¹æ®å›¾æ•°æ®åº“â€¦â€ï¼‰
                already_has_list = "æ ¹æ®å›¾æ•°æ®åº“ï¼Œæ™¯åŒºã€Œ" in (out_context or "")
                if scenic_ctx and not already_has_list:
                    out_context = f"{out_context}\n\n{scenic_ctx}" if out_context else scenic_ctx
                # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°æ™¯åŒºä¿¡æ¯ï¼Œå°è¯•ä» primary_attraction_id åæŸ¥
                elif rag_results.get("primary_attraction_id") is not None and not already_has_list:
                    try:
                        aid = rag_results.get("primary_attraction_id")
                        parent_info = self._get_scenic_spot_by_attraction_id(aid)
                        scenic_name = parent_info.get("s_name") if parent_info else None
                        if scenic_name:
                            scenic_ctx = self._get_scenic_attractions_sentence_by_name(str(scenic_name).strip())
                            if scenic_ctx:
                                out_context = f"{out_context}\n\n{scenic_ctx}" if out_context else scenic_ctx
                    except Exception as e:
                        logger.warning(f"åˆ—ä¸¾æŸ¥è¯¢æ—¶ä»primary_attraction_idåæŸ¥æ™¯åŒºå¤±è´¥: {e}")

            rag_debug = {
                "query": rag_results.get("query") or query,
                "vector_results": rag_results.get("vector_results", [])[:5],
                "graph_results": rag_results.get("graph_results", [])[:5],
                "subgraph": rag_results.get("subgraph"),
                "enhanced_context": out_context or "",
                "entities": rag_results.get("entities", []),
            }
        base_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™¯åŒºAIå¯¼æ¸¸åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨å‹å¥½ã€ä¸“ä¸šã€å‡†ç¡®çš„è¯­è¨€å›ç­”æ¸¸å®¢çš„é—®é¢˜ã€‚
å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”
2. è¯­è¨€ç®€æ´æ˜äº†ï¼Œé€‚åˆå£è¯­åŒ–è¡¨è¾¾
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯šå®è¯´æ˜
4. ä¸è¦ç¼–é€ ä¿¡æ¯
5. ä¸è¦é€éœ²ä»»ä½•å†…éƒ¨æ ‡è¯†ç¬¦/ç¼–å·/IDï¼ˆä¾‹å¦‚ kb_***ã€text_idã€session_id ç­‰ï¼‰ï¼›è‡ªæˆ‘ä»‹ç»æ—¶ä¹Ÿä¸è¦è¾“å‡ºä»»ä½•â€œç¼–å·â€
6. è¾“å‡ºå†…å®¹å¿…é¡»ä¸ºâ€œå¹²å‡€çš„çº¯æ–‡æœ¬â€ï¼šä¸è¦ä½¿ç”¨ä»»ä½•è¡¨æƒ…/emoji/é¢œæ–‡å­—ï¼Œä¹Ÿä¸è¦ä½¿ç”¨è£…é¥°æ€§ç¬¦å·ï¼ˆä¾‹å¦‚ ï½ã€~ã€ğŸ«¶ã€âœ¨ã€â¤ï¸ ç­‰ï¼‰ã€‚åªä½¿ç”¨æ­£å¸¸ä¸­æ–‡æ ‡ç‚¹ï¼ˆï¼Œã€‚ï¼ï¼Ÿï¼‰ä¸å¿…è¦çš„æ•°å­—/å•ä½ã€‚"""
        if character_prompt:
            system_prompt = f"{base_system_prompt}\n\nè§’è‰²è®¾å®šï¼š{character_prompt}"
        else:
            system_prompt = base_system_prompt
        messages = [{"role": "system", "content": system_prompt}]
        if conversation_history:
            messages.extend(conversation_history)
        user_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{query}

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{out_context if out_context else "æ— é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯"}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
        messages.append({"role": "user", "content": user_prompt})
        if rag_debug is not None:
            rag_debug["final_sent_to_llm"] = user_prompt

        try:
            response = self.llm_client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            if answer:
                answer = re.sub(r"ç¼–å·ä¸º\s*kb_\d+", "", answer)
                answer = re.sub(r"\bkb_\d+\b", "", answer)
                answer = re.sub(r"\s{2,}", " ", answer).strip()
            if answer:
                answer = _strip_emoji(answer)
            try:
                log_root = os.path.join(os.path.dirname(os.path.dirname(__file__)), "logs")
                os.makedirs(log_root, exist_ok=True)
                log_path = os.path.join(log_root, "rag_context.log")
                entry = {
                    "timestamp": datetime.utcnow().isoformat() + "Z",
                    "query": query,
                    "character_prompt": character_prompt,
                    "use_rag": use_rag,
                    "rag_debug": rag_debug,
                    "final_answer_preview": answer[:400] if answer else "",
                }
                with open(log_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                try:
                    with open(log_path, "r", encoding="utf-8") as f:
                        lines = [ln for ln in f.readlines() if ln.strip()]
                    if len(lines) > 5:
                        with open(log_path, "w", encoding="utf-8") as f:
                            f.writelines(lines[-5:])
                except Exception:
                    pass
            except Exception as e:
                logger.warning(f"Failed to write RAG context log: {e}")
            logger.info(f"Generated answer for query: {query[:50]}...")
            return {"answer": answer, "primary_attraction_id": primary_attraction_id, "context": out_context}
        except Exception as e:
            logger.error(f"Failed to generate answer: {e}")
            return {"answer": f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}", "primary_attraction_id": None, "context": out_context}


rag_service = RAGService()

