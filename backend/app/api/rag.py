"""GraphRAG æ£€ç´¢ API"""
import base64
import logging
import asyncio
import json
import os
import time
from datetime import datetime
from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional, AsyncGenerator, Tuple
from app.services.rag_service import rag_service, _clean_special_symbols
from app.services.session_service import session_service
from app.services.voice_service import voice_service
from app.api.voice import _normalize_tts_text
from app.core.prisma_client import get_prisma
from app.core.config import settings
from app.models.interaction import Interaction

logger = logging.getLogger(__name__)

router = APIRouter()

class QueryRequest(BaseModel):
    query: str
    top_k: int = 5

class QueryResponse(BaseModel):
    vector_results: List[Dict[str, Any]]
    graph_results: List[Dict[str, Any]]
    query: str

class GenerateRequest(BaseModel):
    query: str
    session_id: Optional[str] = None
    character_id: Optional[int] = None
    use_rag: bool = True
    scenic_name: Optional[str] = None  # ç”¨æˆ·é€‰æ‹©çš„æ™¯åŒºåç§°ï¼Œç”¨äºæŒ‡ä»£æ¶ˆè§£ï¼ˆå¦‚ã€Œè¿™ä¸ªæ™¯åŒºã€ï¼‰

class GenerateResponse(BaseModel):
    answer: str
    query: str
    context: str = ""
    session_id: str


def _resolve_session_id(request: GenerateRequest) -> str:
    """æ ¹æ®è¯·æ±‚è§£ææˆ–åˆ›å»º session_idï¼Œä¾› /generate ä¸ /generate-stream å…±ç”¨ã€‚"""
    if request.session_id:
        session = session_service.get_session(request.session_id)
        if session:
            return request.session_id
    return session_service.create_session(request.character_id)


async def _load_character_prompt_and_voice(character_id: Optional[int]) -> Tuple[Optional[str], Optional[str]]:
    """ä¸€æ¬¡æŸ¥è¯¢è§’è‰²ï¼Œè¿”å› (prompt, voice)ï¼Œä¾› /generate ä¸ /generate-stream å…±ç”¨ï¼Œé¿å…é‡å¤æŸ¥åº“ã€‚"""
    if not character_id:
        return None, None
    try:
        prisma = await get_prisma()
        character = await prisma.character.find_unique(where={"id": character_id})
        if not character:
            return None, None
        prompt = character.prompt if character.prompt else None
        voice = character.voice if character.voice else settings.XFYUN_VOICE
        return prompt, voice
    except Exception as e:
        logger.error("Failed to load character: %s", e)
        return None, None


def _save_interaction(
    session_id: str,
    character_id: Optional[int],
    query_text: str,
    response_text: str,
    primary_attraction_id: Optional[int],
) -> None:
    """ä¿å­˜äº¤äº’è®°å½•åˆ°æ•°æ®åº“ï¼›æœªè¿”å›æ™¯ç‚¹ ID æ—¶æŒ‰é—®é¢˜æ–‡æœ¬å°è¯•åŒ¹é…æ™¯ç‚¹åã€‚ä¾› /generate ä¸ /generate-stream å…±ç”¨ã€‚"""
    try:
        from app.core.database import SessionLocal
        from app.models.attraction import Attraction as AttractionModel
        db_local = SessionLocal()
        try:
            aid = primary_attraction_id
            if aid is None and query_text and query_text.strip():
                q = query_text.strip()
                rows = (
                    db_local.query(AttractionModel.id, AttractionModel.name)
                    .filter(AttractionModel.name.isnot(None), AttractionModel.name != "")
                    .limit(200)
                    .all()
                )
                for row in rows:
                    name = row[1] if len(row) > 1 else None
                    if name and name in q:
                        aid = row[0]
                        break
            interaction = Interaction(
                session_id=session_id,
                character_id=character_id,
                query_text=query_text,
                response_text=response_text,
                interaction_type="voice_query",
                attraction_id=aid,
            )
            db_local.add(interaction)
            db_local.commit()
        finally:
            db_local.close()
    except Exception as e:
        logger.error("Failed to save interaction: %s", e)


@router.post("/search", response_model=QueryResponse)
async def hybrid_search(request: QueryRequest):
    """æ··åˆæ£€ç´¢"""
    try:
        results = await rag_service.hybrid_search(request.query, top_k=request.top_k)
        return QueryResponse(**results)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/vector-search")
async def vector_search(request: QueryRequest):
    """å‘é‡æœç´¢"""
    try:
        results = await rag_service.vector_search(request.query, top_k=request.top_k)
        return {"results": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/graph-search")
async def graph_search(entity_name: str, relation_type: str = None, limit: int = 10):
    """å›¾æœç´¢"""
    try:
        results = await rag_service.graph_search(entity_name, relation_type, limit)
        return {"results": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/generate", response_model=GenerateResponse)
async def generate_answer(request: GenerateRequest, background_tasks: BackgroundTasks):
    """ç”Ÿæˆå›ç­”ï¼ˆRAG + å¤šè½®å¯¹è¯ï¼‰ã€‚"""
    try:
        session_id = _resolve_session_id(request)
        (character_prompt, _), conversation_history = await asyncio.gather(
            _load_character_prompt_and_voice(request.character_id),
            asyncio.to_thread(session_service.get_conversation_history, session_id),
        )

        result = await rag_service.generate_answer(
            query=request.query,
            context=None,
            use_rag=request.use_rag,
            conversation_history=conversation_history,
            character_prompt=character_prompt,
            scenic_name=request.scenic_name,
        )
        answer = result["answer"]
        context = result.get("context", "")
        primary_attraction_id = result.get("primary_attraction_id")

        session_service.add_message(session_id, "user", request.query)
        session_service.add_message(session_id, "assistant", answer)

        background_tasks.add_task(
            _save_interaction,
            session_id,
            request.character_id,
            request.query,
            answer,
            primary_attraction_id,
        )

        return GenerateResponse(
            answer=answer,
            query=request.query,
            context=context,
            session_id=session_id,
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/generate-stream")
async def generate_answer_stream(request: GenerateRequest, background_tasks: BackgroundTasks):
    """æµå¼ç”Ÿæˆå›ç­”ï¼ˆSSEï¼‰ï¼Œæ–‡æœ¬ä¸ TTS åŒæ­¥è¾“å‡º"""
    async def generate_stream() -> AsyncGenerator[str, None]:
        session_id = _resolve_session_id(request)
        (character_prompt, voice), conversation_history = await asyncio.gather(
            _load_character_prompt_and_voice(request.character_id),
            asyncio.to_thread(session_service.get_conversation_history, session_id),
        )
        if not voice:
            voice = settings.XFYUN_VOICE
        
        # æ‰§è¡Œ RAG æ£€ç´¢ï¼ˆéæµå¼ï¼Œä¸€æ¬¡æ€§è·å–ä¸Šä¸‹æ–‡ï¼‰
        rag_results = None
        primary_attraction_id = None
        context = ""
        if request.use_rag:
            try:
                rag_results = await rag_service.hybrid_search(
                    request.query,
                    top_k=5,
                    conversation_history=conversation_history,
                    scenic_name=request.scenic_name,
                )
                primary_attraction_id = rag_results.get("primary_attraction_id")
                context = rag_results.get("enhanced_context", "") or ""
            except Exception as e:
                logger.error(f"RAG search failed: {e}")
                rag_results = {"errors": {"rag_search": str(e)}}
        
        # å‡†å¤‡ LLM æ¶ˆæ¯
        base_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™¯åŒºAIå¯¼æ¸¸åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨å‹å¥½ã€ä¸“ä¸šã€å‡†ç¡®çš„è¯­è¨€å›ç­”æ¸¸å®¢çš„é—®é¢˜ã€‚
å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”
2. è¯­è¨€ç®€æ´æ˜äº†ï¼Œé€‚åˆå£è¯­åŒ–è¡¨è¾¾
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯šå®è¯´æ˜
4. ä¸è¦ç¼–é€ ä¿¡æ¯
5. ä¸è¦é€éœ²ä»»ä½•å†…éƒ¨æ ‡è¯†ç¬¦/ç¼–å·/IDï¼ˆä¾‹å¦‚ kb_***ã€text_idã€session_id ç­‰ï¼‰ï¼›è‡ªæˆ‘ä»‹ç»æ—¶ä¹Ÿä¸è¦è¾“å‡ºä»»ä½•"ç¼–å·"
6. è¾“å‡ºå†…å®¹å¿…é¡»ä¸º"å¹²å‡€çš„çº¯æ–‡æœ¬"ï¼š
   - ç¦æ­¢ä½¿ç”¨ä»»ä½•è¡¨æƒ…ç¬¦å·ã€emojiã€é¢œæ–‡å­—ï¼ˆå¦‚ ğŸŒŸã€âœ¨ã€â¤ï¸ã€ğŸ˜Šã€1ï¸âƒ£ã€2ï¸âƒ£ ç­‰ï¼‰
   - ç¦æ­¢ä½¿ç”¨ Markdown æ ¼å¼ç¬¦å·ï¼ˆå¦‚ **ç²—ä½“**ã€*æ–œä½“*ã€# æ ‡é¢˜ã€- åˆ—è¡¨ç¬¦å·ç­‰ï¼‰
   - ç¦æ­¢ä½¿ç”¨è£…é¥°æ€§ç¬¦å·ï¼ˆå¦‚ ï½ã€~ã€â€”â€”ã€â€¦ã€â€¢ã€â–ªã€â–« ç­‰ï¼‰
   - åªä½¿ç”¨æ­£å¸¸ä¸­æ–‡æ ‡ç‚¹ï¼ˆï¼Œã€‚ï¼ï¼Ÿï¼šï¼›ï¼‰ä¸å¿…è¦çš„æ•°å­—ã€å•ä½
   - å¦‚éœ€åˆ—ä¸¾ï¼Œä½¿ç”¨"ç¬¬ä¸€"ã€"ç¬¬äºŒ"æˆ–"1."ã€"2."ç­‰çº¯æ–‡æœ¬æ ¼å¼ï¼Œä¸è¦ç”¨ç‰¹æ®Šç¬¦å·"""
        
        if character_prompt:
            system_prompt = f"{base_system_prompt}\n\nè§’è‰²è®¾å®šï¼š{character_prompt}"
        else:
            system_prompt = base_system_prompt

        # æ˜¯å¦åœ¨åç«¯åš TTSï¼ˆç§‘å¤§è®¯é£æˆ–æœ¬åœ° CosyVoice2ï¼‰ï¼Œè¾¹ç”Ÿæˆè¾¹åˆæˆ
        backend_tts_enabled = bool(settings.XFYUN_APPID and settings.XFYUN_API_KEY) or settings.LOCAL_TTS_ENABLED
        
        messages = [{"role": "system", "content": system_prompt}]
        if conversation_history:
            messages.extend(conversation_history)
        
        user_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{request.query}
ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context if context else "æ— é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯"}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
        messages.append({"role": "user", "content": user_prompt})
        
        # æµå¼è°ƒç”¨ LLM
        try:
            if not rag_service.llm_client:
                yield f"data: {json.dumps({'type': 'error', 'content': 'AIæœåŠ¡æœªé…ç½®'}, ensure_ascii=False)}\n\n"
                return
            
            # ä½¿ç”¨æµå¼ API
            stream = rag_service.llm_client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
                stream=True
            )
            
            full_answer = ""
            accumulated_text = ""
            completed_audio: Dict[int, str] = {}
            fallback_tts: Dict[int, str] = {}  # åç«¯ TTS å¤±è´¥æ—¶ï¼ŒæŒ‰å¥å›é€€ä¸º tts æ–‡æœ¬ç»™å‰ç«¯åˆæˆ
            next_audio_idx = 0
            tts_chunk_index = [0]
            MIN_TTS_CHARS = 12
            TTS_SENTENCE_TIMEOUT = 25  # å•å¥åˆæˆè¶…æ—¶ï¼ˆç§’ï¼‰ï¼Œé¿å…æŸå¥å¡ä½æ‹–æ­»æ•´æ®µ
            
            async def synthesize_and_store(idx: int, original_txt: str) -> None:
                txt = _normalize_tts_text(original_txt)
                if not txt:
                    completed_audio[idx] = ""
                    return
                path = None
                try:
                    if not settings.LOCAL_TTS_FORCE:
                        try:
                            path = await asyncio.wait_for(
                                voice_service.synthesize_xfyun(txt, voice=voice),
                                timeout=TTS_SENTENCE_TIMEOUT,
                            )
                        except asyncio.TimeoutError:
                            logger.debug("ç§‘å¤§è®¯é£ TTS å•å¥è¶…æ—¶(%ds)", TTS_SENTENCE_TIMEOUT)
                        except Exception as e:
                            logger.debug("ç§‘å¤§è®¯é£ TTS å¤±è´¥: %s", e)
                    if path is None and settings.LOCAL_TTS_ENABLED:
                        try:
                            path = await asyncio.wait_for(
                                voice_service.synthesize_local_cosyvoice2(txt, voice=voice),
                                timeout=TTS_SENTENCE_TIMEOUT,
                            )
                        except asyncio.TimeoutError:
                            logger.debug("CosyVoice2 TTS å•å¥è¶…æ—¶")
                        except Exception as e:
                            logger.debug("CosyVoice2 TTS å¤±è´¥: %s", e)
                    if path and os.path.exists(path):
                        with open(path, "rb") as f:
                            b64 = base64.b64encode(f.read()).decode("utf-8")
                        completed_audio[idx] = b64
                        try:
                            os.unlink(path)
                        except OSError:
                            pass
                    else:
                        fallback_tts[idx] = original_txt.strip()
                        completed_audio[idx] = ""
                except Exception as e:
                    logger.debug("æµå¼ TTS åˆæˆå¤±è´¥: %s", e)
                    fallback_tts[idx] = original_txt.strip()
                    completed_audio[idx] = ""
            
            def drain_audio():
                nonlocal next_audio_idx
                while next_audio_idx in completed_audio:
                    idx = next_audio_idx
                    b64 = completed_audio.pop(idx)
                    next_audio_idx += 1
                    if b64:
                        yield f"data: {json.dumps({'type': 'audio', 'content': b64}, ensure_ascii=False)}\n\n"
                    else:
                        # åç«¯åˆæˆå¤±è´¥ï¼ŒæŒ‰å¥å›é€€ï¼šå‘ tts æ–‡æœ¬è®©å‰ç«¯ POST åˆæˆ
                        if idx in fallback_tts:
                            text = fallback_tts.pop(idx)
                            if text:
                                yield f"data: {json.dumps({'type': 'tts', 'content': text}, ensure_ascii=False)}\n\n"
            
            # å‘é€ session_id
            yield f"data: {json.dumps({'type': 'session_id', 'content': session_id}, ensure_ascii=False)}\n\n"
            
            # å‘é€ primary_attraction_idï¼ˆç”¨äºåç»­ä¿å­˜äº¤äº’ï¼‰
            yield f"data: {json.dumps({'type': 'attraction_id', 'content': primary_attraction_id}, ensure_ascii=False)}\n\n"
            
            # ç”¨é˜Ÿåˆ— + åå°æ¶ˆè´¹ streamï¼Œä¸»å¾ªç¯æ¯ 50ms æ£€æŸ¥ä¸€æ¬¡ï¼šæœ‰ chunk å°±å¤„ç†å¹¶ yield æ–‡æœ¬ï¼Œæ—  chunk å°± drain å·²å°±ç»ªçš„éŸ³é¢‘å¹¶ yieldï¼Œå®ç°ã€Œè¾¹å‡ºå­—è¾¹å‡ºå£°éŸ³ã€
            chunk_queue: asyncio.Queue = asyncio.Queue()
            loop = asyncio.get_event_loop()
            stream_sentinel = object()
            
            def put_stream_in_queue():
                try:
                    for c in stream:
                        loop.call_soon_threadsafe(chunk_queue.put_nowait, c)
                finally:
                    loop.call_soon_threadsafe(chunk_queue.put_nowait, stream_sentinel)
            
            loop.run_in_executor(None, put_stream_in_queue)
            DRAIN_INTERVAL = 0.05
            
            while True:
                try:
                    chunk = await asyncio.wait_for(chunk_queue.get(), timeout=DRAIN_INTERVAL)
                except asyncio.TimeoutError:
                    for ev in drain_audio():
                        yield ev
                    continue
                if chunk is stream_sentinel:
                    break
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        content = delta.content
                        content = _clean_special_symbols(content)
                        if not content:
                            continue
                        full_answer += content
                        accumulated_text += content
                        
                        tts_chunk = None
                        if any(punct in accumulated_text for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']):
                            last_punct_idx = max(
                                accumulated_text.rfind('ã€‚'),
                                accumulated_text.rfind('ï¼'),
                                accumulated_text.rfind('ï¼Ÿ'),
                                accumulated_text.rfind('.'),
                                accumulated_text.rfind('!'),
                                accumulated_text.rfind('?')
                            )
                            if last_punct_idx >= 0:
                                tts_chunk = accumulated_text[:last_punct_idx + 1]
                                accumulated_text = accumulated_text[last_punct_idx + 1:]
                        elif len(accumulated_text) >= MIN_TTS_CHARS:
                            tts_chunk = accumulated_text
                            accumulated_text = ""
                        
                        if tts_chunk:
                            if backend_tts_enabled:
                                idx = tts_chunk_index[0]
                                tts_chunk_index[0] += 1
                                asyncio.create_task(synthesize_and_store(idx, tts_chunk))
                            else:
                                yield f"data: {json.dumps({'type': 'tts', 'content': tts_chunk}, ensure_ascii=False)}\n\n"
                        
                        yield f"data: {json.dumps({'type': 'text', 'content': content}, ensure_ascii=False)}\n\n"
                
                # æ¯å¤„ç†å®Œä¸€ä¸ª chunk æˆ–è¶…æ—¶éƒ½ drain å·²å°±ç»ªçš„éŸ³é¢‘ï¼Œå®ç°è¾¹å‡ºå­—è¾¹æ’­æ”¾
                for ev in drain_audio():
                    yield ev
            
            # å‰©ä½™ç´¯ç§¯æ–‡æœ¬
            if accumulated_text.strip():
                if backend_tts_enabled:
                    idx = tts_chunk_index[0]
                    tts_chunk_index[0] += 1
                    asyncio.create_task(synthesize_and_store(idx, accumulated_text.strip()))
                else:
                    yield f"data: {json.dumps({'type': 'tts', 'content': accumulated_text.strip()}, ensure_ascii=False)}\n\n"
            
            # å…ˆå†™å…¥ RAG æ—¥å¿—å¹¶ä¿å­˜äº¤äº’ï¼Œç®¡ç†ç«¯å¯ç«‹å³çœ‹åˆ°æ›´æ–°ï¼ˆä¸å¿…ç­‰ TTS å…¨éƒ¨æ’­å®Œï¼‰
            try:
                rag_debug: Optional[Dict[str, Any]] = None
                if request.use_rag:
                    rag_debug = {
                        "query": (rag_results or {}).get("query") or request.query,
                        "vector_results": ((rag_results or {}).get("vector_results") or [])[:5],
                        "graph_results": ((rag_results or {}).get("graph_results") or [])[:5],
                        "subgraph": (rag_results or {}).get("subgraph"),
                        "enhanced_context": context or "",
                        "entities": (rag_results or {}).get("entities", []),
                        "errors": (rag_results or {}).get("errors", {}),
                        "intent": (rag_results or {}).get("intent"),
                        "strategy": (rag_results or {}).get("strategy"),
                        "final_sent_to_llm": user_prompt,
                    }
                else:
                    rag_debug = {
                        "query": request.query,
                        "vector_results": [],
                        "graph_results": [],
                        "subgraph": None,
                        "enhanced_context": "",
                        "entities": [],
                        "skip_rag_reason": "æœªä½¿ç”¨ RAG",
                        "final_sent_to_llm": user_prompt,
                    }
                log_root = os.path.join(os.path.dirname(os.path.dirname(__file__)), "logs")
                os.makedirs(log_root, exist_ok=True)
                log_path = os.path.join(log_root, "rag_context.log")
                entry = {
                    "timestamp": datetime.utcnow().isoformat() + "Z",
                    "query": request.query,
                    "character_prompt": character_prompt,
                    "use_rag": request.use_rag,
                    "rag_debug": rag_debug,
                    "final_answer_preview": (full_answer or "")[:400],
                }
                with open(log_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                try:
                    with open(log_path, "r", encoding="utf-8") as f:
                        lines = [ln for ln in f.readlines() if ln.strip()]
                    if len(lines) > 5:
                        with open(log_path, "w", encoding="utf-8") as f:
                            f.writelines(lines[-5:])
                except Exception:
                    pass
            except Exception as e:
                logger.warning(f"Failed to write RAG context log (stream): {e}")
            
            session_service.add_message(session_id, "user", request.query)
            session_service.add_message(session_id, "assistant", full_answer)

            await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: _save_interaction(
                    session_id,
                    request.character_id,
                    request.query,
                    full_answer,
                    primary_attraction_id,
                ),
            )
            
            # ç­‰å¾…æ‰€æœ‰ TTS å®Œæˆå¹¶æŒç»­ drain éŸ³é¢‘ï¼ˆè¾¹ç­‰è¾¹ yieldï¼Œç”¨æˆ·èƒ½è¾¹å¬ï¼‰
            wait_start = time.monotonic()
            while next_audio_idx < tts_chunk_index[0] or completed_audio:
                if time.monotonic() - wait_start > 60:
                    logger.debug("æµå¼ TTS ç­‰å¾…è¶…æ—¶")
                    break
                await asyncio.sleep(DRAIN_INTERVAL)
                for ev in drain_audio():
                    yield ev
            
            # å‘é€å®Œæˆä¿¡å·
            yield f"data: {json.dumps({'type': 'done', 'content': full_answer}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            logger.error(f"Stream generation failed: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(generate_stream(), media_type="text/event-stream")

